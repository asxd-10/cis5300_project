{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acewMLSaDIFa"
      },
      "source": [
        "# Extension 2: PubMedBERT with Hard Negatives and Focal Loss\n",
        "\n",
        "## Context: Starting from PubMedBERT Baseline\n",
        "\n",
        "In the baseline, we achieved **39.30% sentence-level F1** using PubMedBERT with sentence-pair architecture:\n",
        "- Input: `[CLS] claim [SEP] sentence [SEP]`\n",
        "- Model: PubMedBERT encoder + evidence head (binary) + claim classifier (3-way)\n",
        "- Training: All claims including NEI, but evidence loss uses standard BCE\n",
        "\n",
        "## Problem Identified\n",
        "\n",
        "While the baseline already processes NEI claims, there are opportunities for improvement:\n",
        "\n",
        "1. **Evidence Loss Imbalance**: Standard BCE struggles with class imbalance (most sentences are non-evidence)\n",
        "2. **Inference Rule**: Forcing \"no evidence ⇒ NEI\" can override correct stance predictions\n",
        "3. **Negative Quality**: Current approach uses random claim+random doc pairings, which can be noisy\n",
        "\n",
        "## Proposed Solution\n",
        "\n",
        "This extension implements three targeted improvements:\n",
        "\n",
        "1. **Better Negatives**: Use real NEI examples from NEI claims with their cited documents (not random pairings)\n",
        "2. **Focal Loss**: Replace standard BCE with Focal Loss to better handle evidence class imbalance\n",
        "3. **Cleaner Inference**: Remove \"no evidence ⇒ NEI\" forcing, let stance classifier decide\n",
        "4. **Optional Hard Negatives**: Lexical similarity-based negatives from similar but non-gold documents\n",
        "\n",
        "## Expected Impact\n",
        "\n",
        "- **Target**: +2-4% F1 improvement (from 39.30% to 41-43%)\n",
        "- **Primary benefit**: Better precision through improved evidence loss\n",
        "- **Secondary benefit**: More accurate NEI classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWkggK8dD305",
        "outputId": "708f4ad9-fde5-4f59-89fe-ab8b08788bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.4.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U0XGcDhjDIFg"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers torch jsonlines tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPIut9CADIFd",
        "outputId": "9f17dae5-ea26-4350-80ed-3d506b8685e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Setup: Mount Google Drive and install dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import jsonlines\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIwOwQZPDIFg",
        "outputId": "31e0673f-71da-4614-e56a-15011f5cf2bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cis5300_project'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 286 (delta 160), reused 97 (delta 30), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (286/286), 14.30 MiB | 15.77 MiB/s, done.\n",
            "Resolving deltas: 100% (160/160), done.\n",
            "Changed to: /content/cis5300_project\n"
          ]
        }
      ],
      "source": [
        "# Navigate to project directory\n",
        "# PROJECT_PATH = '/content/drive/MyDrive/cis5300_project'\n",
        "# Or if cloned from GitHub:\n",
        "!git clone https://github.com/asxd-10/cis5300_project.git\n",
        "PROJECT_PATH = '/content/cis5300_project'\n",
        "\n",
        "if os.path.exists(PROJECT_PATH):\n",
        "    os.chdir(PROJECT_PATH)\n",
        "    print(f\"Changed to: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"Project path not found: {PROJECT_PATH}\")\n",
        "    print(\"Please update PROJECT_PATH or clone the repository\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxvk-KV0DIFi",
        "outputId": "4d2abde1-b577-4fd4-d3b1-b374f6e63cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
            "  Device: cuda\n",
            "  Focal Loss: alpha=0.75, gamma=2.0\n",
            "  Hard Negatives: True, ratio=0.3\n",
            "  Batch Size: 16\n",
            "  Learning Rate: 2e-05\n",
            "  Epochs: 6\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16  # Same as baseline\n",
        "LEARNING_RATE = 2e-5  # Same as baseline\n",
        "NUM_EPOCHS = 6  # Same as baseline\n",
        "EVIDENCE_LOSS_WEIGHT = 2.0  # Weight for evidence loss\n",
        "\n",
        "# Focal Loss Configuration\n",
        "FOCAL_ALPHA = 0.75  # Weight for positive class\n",
        "FOCAL_GAMMA = 2.0   # Focusing parameter\n",
        "\n",
        "# Hard Negative Mining Configuration\n",
        "USE_LEXICAL_HARD_NEGATIVES = True  # Enable lexical similarity-based hard negatives\n",
        "HARD_NEGATIVE_RATIO = 0.3  # Add 30% hard negatives relative to gold examples\n",
        "MAX_HARD_NEGATIVES_PER_CLAIM = 5  # Max hard negative sentences per claim\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Focal Loss: alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA}\")\n",
        "print(f\"  Hard Negatives: {USE_LEXICAL_HARD_NEGATIVES}, ratio={HARD_NEGATIVE_RATIO}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WHEjy0eNv4s",
        "outputId": "10a17422-f2b3-48d2-ef15-b8e81c7bbe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "809 training claims\n",
            "300 dev claims\n",
            "5183 documents\n",
            "\n",
            "============================================================\n",
            "EXPLORATORY DATA ANALYSIS\n",
            "============================================================\n",
            "\n",
            "1. Training Data Breakdown:\n",
            "   Claims WITH evidence: 505 (62.4%)\n",
            "   NOT_ENOUGH_INFO claims: 304 (37.6%)\n",
            "   Total: 809\n",
            "\n",
            "2. Label Distribution (claims with evidence):\n",
            "   SUPPORT: 332 (65.7%)\n",
            "   CONTRADICT: 173 (34.3%)\n",
            "\n",
            "3. Evidence Sentence Statistics:\n",
            "   Mean evidence sentences per claim: 2.03\n",
            "   Min: 1, Max: 11\n",
            "   Claims with 1 sentence: 225\n",
            "   Claims with 2+ sentences: 280\n",
            "\n",
            "4. Document Statistics:\n",
            "   Mean sentences per document: 8.87\n",
            "   Min: 3, Max: 367\n",
            "\n",
            "5. Class Imbalance Analysis:\n",
            "   Evidence sentences per document: ~2.03\n",
            "   Total sentences per document: ~8.87\n",
            "   Imbalance ratio: ~4.4:1 (non-evidence : evidence)\n",
            "   This explains why standard BCE loss struggles!\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "from src.common.data_utils import load_claims, load_corpus\n",
        "from collections import Counter\n",
        "\n",
        "train_claims = load_claims('data/scifact/data/claims_train.jsonl')\n",
        "dev_claims = load_claims('data/scifact/data/claims_dev.jsonl')\n",
        "corpus = load_corpus('data/scifact/data/corpus.jsonl')\n",
        "\n",
        "print(f\"{len(train_claims)} training claims\")\n",
        "print(f\"{len(dev_claims)} dev claims\")\n",
        "print(f\"{len(corpus)} documents\")\n",
        "\n",
        "# ============================================================\n",
        "# EXPLORATORY DATA ANALYSIS\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analyze data distribution\n",
        "claims_with_evidence = [c for c in train_claims if c.evidence and c.label]\n",
        "nei_claims = [c for c in train_claims if not c.evidence or c.label == 'NOT_ENOUGH_INFO']\n",
        "\n",
        "print(f\"\\n1. Training Data Breakdown:\")\n",
        "print(f\"   Claims WITH evidence: {len(claims_with_evidence)} ({100*len(claims_with_evidence)/len(train_claims):.1f}%)\")\n",
        "print(f\"   NOT_ENOUGH_INFO claims: {len(nei_claims)} ({100*len(nei_claims)/len(train_claims):.1f}%)\")\n",
        "print(f\"   Total: {len(train_claims)}\")\n",
        "\n",
        "# Label distribution\n",
        "label_counts = Counter([c.label for c in claims_with_evidence if c.label])\n",
        "print(f\"\\n2. Label Distribution (claims with evidence):\")\n",
        "for label, count in label_counts.most_common():\n",
        "    print(f\"   {label}: {count} ({100*count/len(claims_with_evidence):.1f}%)\")\n",
        "\n",
        "# Evidence sentence statistics\n",
        "evidence_sent_counts = []\n",
        "for claim in claims_with_evidence:\n",
        "    total_ev_sents = 0\n",
        "    for doc_id, ev_list in claim.evidence.items():\n",
        "        for ev_entry in ev_list:\n",
        "            total_ev_sents += len(ev_entry.get('sentences', []))\n",
        "    evidence_sent_counts.append(total_ev_sents)\n",
        "\n",
        "if evidence_sent_counts:\n",
        "    print(f\"\\n3. Evidence Sentence Statistics:\")\n",
        "    print(f\"   Mean evidence sentences per claim: {sum(evidence_sent_counts)/len(evidence_sent_counts):.2f}\")\n",
        "    print(f\"   Min: {min(evidence_sent_counts)}, Max: {max(evidence_sent_counts)}\")\n",
        "    print(f\"   Claims with 1 sentence: {sum(1 for x in evidence_sent_counts if x == 1)}\")\n",
        "    print(f\"   Claims with 2+ sentences: {sum(1 for x in evidence_sent_counts if x >= 2)}\")\n",
        "\n",
        "# Document length statistics\n",
        "doc_lengths = [len(doc.abstract) for doc in corpus.values()]\n",
        "print(f\"\\n4. Document Statistics:\")\n",
        "print(f\"   Mean sentences per document: {sum(doc_lengths)/len(doc_lengths):.2f}\")\n",
        "print(f\"   Min: {min(doc_lengths)}, Max: {max(doc_lengths)}\")\n",
        "\n",
        "# Class imbalance analysis\n",
        "print(f\"\\n5. Class Imbalance Analysis:\")\n",
        "print(f\"   Evidence sentences per document: ~{sum(evidence_sent_counts)/len(evidence_sent_counts) if evidence_sent_counts else 0:.2f}\")\n",
        "print(f\"   Total sentences per document: ~{sum(doc_lengths)/len(doc_lengths):.2f}\")\n",
        "imbalance_ratio = (sum(doc_lengths)/len(doc_lengths)) / (sum(evidence_sent_counts)/len(evidence_sent_counts) if evidence_sent_counts else 1)\n",
        "print(f\"   Imbalance ratio: ~{imbalance_ratio:.1f}:1 (non-evidence : evidence)\")\n",
        "print(f\"   This explains why standard BCE loss struggles!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzg4zPiPMyG7",
        "outputId": "6bcc143e-b67e-4e78-80b6-5f2114303814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Focal Loss class defined\n"
          ]
        }
      ],
      "source": [
        "# Focal Loss for Evidence Classification\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for addressing class imbalance in evidence classification.\n",
        "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logits: [N] - evidence logits\n",
        "            targets: [N] - evidence labels (0 or 1)\n",
        "        \"\"\"\n",
        "        # Compute BCE loss\n",
        "        bce_loss = self.bce(logits, targets.float())\n",
        "\n",
        "        # Compute p_t (probability of true class)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
        "\n",
        "        # Compute focal weight\n",
        "        focal_weight = (1 - p_t) ** self.gamma\n",
        "\n",
        "        # Compute alpha_t (class weighting)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "\n",
        "        # Compute focal loss\n",
        "        focal_loss = alpha_t * focal_weight * bce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "print(\"Focal Loss class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulh62VBuMyG8",
        "outputId": "ea291ea7-44b1-4c9e-b7fd-4406a50e230c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lexical filter function defined\n"
          ]
        }
      ],
      "source": [
        "# Lexical similarity filter for hard negatives\n",
        "def simple_lexical_filter(claim_text, corpus, exclude_doc_ids, max_candidates=10):\n",
        "    \"\"\"\n",
        "    Find documents that share tokens with the claim (simple lexical overlap).\n",
        "    Returns list of candidate doc_ids that are similar but not in exclude_doc_ids.\n",
        "    \"\"\"\n",
        "    # Simple tokenization (lowercase, split)\n",
        "    claim_tokens = set(claim_text.lower().split())\n",
        "    # Remove very common words (stopwords-like)\n",
        "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
        "    claim_tokens = claim_tokens - stopwords\n",
        "\n",
        "    if len(claim_tokens) == 0:\n",
        "        return []\n",
        "\n",
        "    candidates = []\n",
        "    for doc_id, doc in corpus.items():\n",
        "        if doc_id in exclude_doc_ids:\n",
        "            continue\n",
        "\n",
        "        # Tokenize document (title + abstract)\n",
        "        doc_text = (doc.title + \" \" + \" \".join(doc.abstract)).lower()\n",
        "        doc_tokens = set(doc_text.split())\n",
        "\n",
        "        # Check overlap\n",
        "        overlap = claim_tokens & doc_tokens\n",
        "        if len(overlap) > 0:\n",
        "            candidates.append(doc_id)\n",
        "            if len(candidates) >= max_candidates:\n",
        "                break\n",
        "\n",
        "    return candidates\n",
        "\n",
        "print(\"Lexical filter function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xelIv1UVDIFi",
        "outputId": "507c5c85-cef0-495a-d43e-ccd04bd1fc79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved dataset class defined\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Dataset with Better Negatives\n",
        "class ImprovedSciFactSentencePairDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Sentence-pair dataset with improved negative mining:\n",
        "    1. Real NEI examples from NEI claims with their cited documents (not random pairings)\n",
        "    2. Optional lexical hard negatives from similar but non-gold documents\n",
        "    3. All positive examples from gold documents (same as baseline)\n",
        "    \"\"\"\n",
        "    def __init__(self, claims, corpus, tokenizer, max_len=256, mode='train',\n",
        "                 use_lexical_hard_negatives=True, hard_negative_ratio=0.3, max_hard_negatives_per_claim=5):\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.label_map = {'SUPPORT': 0, 'CONTRADICT': 1, 'NOT_ENOUGH_INFO': 2}\n",
        "        self.use_lexical_hard_negatives = use_lexical_hard_negatives and (mode == 'train')\n",
        "        self.hard_negative_ratio = hard_negative_ratio\n",
        "        self.max_hard_negatives_per_claim = max_hard_negatives_per_claim\n",
        "\n",
        "        # Separate claims\n",
        "        claims_with_evidence = []\n",
        "        nei_claims = []\n",
        "\n",
        "        for claim in claims:\n",
        "            if claim.evidence and len(claim.evidence) > 0:\n",
        "                claims_with_evidence.append(claim)\n",
        "            elif claim.label == 'NOT_ENOUGH_INFO':\n",
        "                nei_claims.append(claim)\n",
        "\n",
        "        print(f\"  Claims with evidence: {len(claims_with_evidence)}\")\n",
        "        print(f\"  NOT_ENOUGH_INFO claims: {len(nei_claims)}\")\n",
        "\n",
        "        num_positive = 0\n",
        "        num_local_negative = 0\n",
        "        num_nei_negative = 0\n",
        "        num_hard_negative = 0\n",
        "\n",
        "        # 1. Positive examples + local negatives from gold documents\n",
        "        for claim in claims_with_evidence:\n",
        "            gold_doc_ids = set()\n",
        "\n",
        "            for doc_id in claim.cited_doc_ids:\n",
        "                doc_int = int(doc_id)\n",
        "                if doc_int not in corpus:\n",
        "                    continue\n",
        "                doc = corpus[doc_int]\n",
        "                gold_doc_ids.add(doc_int)\n",
        "\n",
        "                for sent_idx, sent in enumerate(doc.abstract):\n",
        "                    # Check if this sentence is evidence\n",
        "                    is_evidence = 0\n",
        "                    evidence_label = 'NOT_ENOUGH_INFO'\n",
        "                    if claim.evidence and str(doc.doc_id) in claim.evidence:\n",
        "                        for ev in claim.evidence[str(doc.doc_id)]:\n",
        "                            if sent_idx in ev.get('sentences', []):\n",
        "                                is_evidence = 1\n",
        "                                evidence_label = ev.get('label')\n",
        "                                break\n",
        "\n",
        "                    claim_label = self.label_map.get(claim.label, 2)\n",
        "\n",
        "                    self.examples.append({\n",
        "                        'claim_id': claim.id,\n",
        "                        'doc_id': doc.doc_id,\n",
        "                        'sent_idx': sent_idx,\n",
        "                        'claim': claim.claim,\n",
        "                        'sentence': sent,\n",
        "                        'is_evidence': is_evidence,\n",
        "                        'claim_label': claim_label,\n",
        "                        'evidence_label_str': evidence_label\n",
        "                    })\n",
        "\n",
        "                    if is_evidence:\n",
        "                        num_positive += 1\n",
        "                    else:\n",
        "                        num_local_negative += 1\n",
        "\n",
        "            # 2. Add lexical hard negatives for this claim\n",
        "            if self.use_lexical_hard_negatives:\n",
        "                # Count gold sentences for this claim\n",
        "                num_gold_sents = sum(len(corpus[int(d)].abstract) for d in claim.cited_doc_ids if int(d) in corpus)\n",
        "                num_hard_needed = min(int(num_gold_sents * self.hard_negative_ratio), self.max_hard_negatives_per_claim)\n",
        "\n",
        "                if num_hard_needed > 0:\n",
        "                    # Find similar documents\n",
        "                    candidate_docs = simple_lexical_filter(claim.claim, corpus, gold_doc_ids, max_candidates=20)\n",
        "\n",
        "                    if candidate_docs:\n",
        "                        # Sample one candidate document\n",
        "                        selected_doc_id = random.choice(candidate_docs)\n",
        "                        selected_doc = corpus[selected_doc_id]\n",
        "\n",
        "                        # Sample sentences from this document\n",
        "                        num_sents_to_sample = min(num_hard_needed, len(selected_doc.abstract))\n",
        "                        sent_indices = random.sample(range(len(selected_doc.abstract)), num_sents_to_sample)\n",
        "\n",
        "                        for sent_idx in sent_indices:\n",
        "                            self.examples.append({\n",
        "                                'claim_id': claim.id,\n",
        "                                'doc_id': selected_doc.doc_id,\n",
        "                                'sent_idx': sent_idx,\n",
        "                                'claim': claim.claim,\n",
        "                                'sentence': selected_doc.abstract[sent_idx],\n",
        "                                'is_evidence': 0,  # Hard negative: no evidence\n",
        "                                'claim_label': 2,  # NOT_ENOUGH_INFO\n",
        "                                'evidence_label_str': 'NOT_ENOUGH_INFO'\n",
        "                            })\n",
        "                            num_hard_negative += 1\n",
        "\n",
        "        # 3. Real NEI examples: NEI claims with their cited documents\n",
        "        for claim in nei_claims:\n",
        "            if not claim.cited_doc_ids:\n",
        "                # If no cited docs, optionally sample random docs (but limit this)\n",
        "                if mode == 'train' and random.random() < 0.3:  # Only 30% of NEI claims without cited docs\n",
        "                    available_docs = [d for d in corpus.keys()]\n",
        "                    if available_docs:\n",
        "                        sampled_doc_id = random.choice(available_docs)\n",
        "                        sampled_doc = corpus[sampled_doc_id]\n",
        "                        # Sample a few sentences\n",
        "                        num_sents = min(3, len(sampled_doc.abstract))\n",
        "                        for sent_idx in range(num_sents):\n",
        "                            self.examples.append({\n",
        "                                'claim_id': claim.id,\n",
        "                                'doc_id': sampled_doc.doc_id,\n",
        "                                'sent_idx': sent_idx,\n",
        "                                'claim': claim.claim,\n",
        "                                'sentence': sampled_doc.abstract[sent_idx],\n",
        "                                'is_evidence': 0,\n",
        "                                'claim_label': 2,\n",
        "                                'evidence_label_str': 'NOT_ENOUGH_INFO'\n",
        "                            })\n",
        "                            num_nei_negative += 1\n",
        "                continue\n",
        "\n",
        "            # Process cited documents for NEI claims (REAL NEI examples, not random)\n",
        "            for doc_id in claim.cited_doc_ids:\n",
        "                doc_int = int(doc_id)\n",
        "                if doc_int not in corpus:\n",
        "                    continue\n",
        "                doc = corpus[doc_int]\n",
        "\n",
        "                # All sentences from NEI claims are non-evidence\n",
        "                for sent_idx, sent in enumerate(doc.abstract):\n",
        "                    self.examples.append({\n",
        "                        'claim_id': claim.id,\n",
        "                        'doc_id': doc.doc_id,\n",
        "                        'sent_idx': sent_idx,\n",
        "                        'claim': claim.claim,\n",
        "                        'sentence': sent,\n",
        "                        'is_evidence': 0,  # All non-evidence for NEI claims\n",
        "                        'claim_label': 2,  # NOT_ENOUGH_INFO\n",
        "                        'evidence_label_str': 'NOT_ENOUGH_INFO'\n",
        "                    })\n",
        "                    num_nei_negative += 1\n",
        "\n",
        "        print(f\"  Total examples: {len(self.examples)}\")\n",
        "        print(f\"    Positive (evidence=1): {num_positive}\")\n",
        "        print(f\"    Local negatives (non-evidence in gold docs): {num_local_negative}\")\n",
        "        print(f\"    NEI negatives (from NEI claims with cited docs): {num_nei_negative}\")\n",
        "        print(f\"    Hard negatives (lexical similarity): {num_hard_negative}\")\n",
        "\n",
        "        # Print evidence distribution\n",
        "        evidence_counts = Counter([ex['is_evidence'] for ex in self.examples])\n",
        "        print(f\"  Evidence distribution: {dict(evidence_counts)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        # Tokenize as pair: [CLS] claim [SEP] sentence [SEP]\n",
        "        encoding = self.tokenizer(\n",
        "            ex['claim'], ex['sentence'],\n",
        "            truncation='only_second',  # Truncate sentence, not claim\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item['is_evidence'] = torch.tensor(ex['is_evidence'], dtype=torch.float)\n",
        "        item['claim_label'] = torch.tensor(ex['claim_label'], dtype=torch.long)\n",
        "        item['claim_id'] = ex['claim_id']\n",
        "        item['doc_id'] = ex['doc_id']\n",
        "        item['sent_idx'] = ex['sent_idx']\n",
        "        return item\n",
        "\n",
        "print(\"Improved dataset class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJtFzEMDIFk",
        "outputId": "cb68715e-9d2c-412b-e22a-86b44bee21bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Distribution:\n",
            "  Labels: {'NOT_ENOUGH_INFO': 304, 'CONTRADICT': 173, 'SUPPORT': 332}\n",
            "  Has Evidence: {False: 304, True: 505}\n",
            "\n",
            "  NOT_ENOUGH_INFO claims: 304\n",
            "  Claims with evidence: 505\n",
            "  Claims without evidence: 304\n"
          ]
        }
      ],
      "source": [
        "# Analyze training data distribution\n",
        "train_labels = [c.label for c in train_claims]\n",
        "train_has_evidence = [len(c.evidence) > 0 for c in train_claims]\n",
        "\n",
        "label_counts = Counter(train_labels)\n",
        "evidence_counts = Counter(train_has_evidence)\n",
        "\n",
        "print(\"Training Data Distribution:\")\n",
        "print(f\"  Labels: {dict(label_counts)}\")\n",
        "print(f\"  Has Evidence: {dict(evidence_counts)}\")\n",
        "print(f\"\\n  NOT_ENOUGH_INFO claims: {label_counts.get('NOT_ENOUGH_INFO', 0)}\")\n",
        "print(f\"  Claims with evidence: {evidence_counts.get(True, 0)}\")\n",
        "print(f\"  Claims without evidence: {evidence_counts.get(False, 0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQxkqa1DIFk",
        "outputId": "c7469e5e-9cae-4dc2-ba85-acc20d2e172d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset class defined with hard negative mining!\n"
          ]
        }
      ],
      "source": [
        "# EXTENSION 1: Enhanced Dataset with Hard Negative Mining\n",
        "class SciFactSentencePairDatasetWithNegatives(Dataset):\n",
        "    \"\"\"\n",
        "    Sentence-pair dataset with hard negative mining.\n",
        "    Creates (claim, sentence) examples for sentences from cited docs.\n",
        "    Adds negative examples from NOT_ENOUGH_INFO claims paired with random documents.\n",
        "    \"\"\"\n",
        "    def __init__(self, claims, corpus, tokenizer, max_len=256, mode='train', negative_ratio=0.5):\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.mode = mode\n",
        "        self.label_map = {'SUPPORT': 0, 'CONTRADICT': 1, 'NOT_ENOUGH_INFO': 2}\n",
        "\n",
        "        # Separate claims with and without evidence\n",
        "        claims_with_evidence = []\n",
        "        nei_claims = []\n",
        "\n",
        "        for claim in claims:\n",
        "            if claim.evidence and len(claim.evidence) > 0:\n",
        "                claims_with_evidence.append(claim)\n",
        "            elif claim.label == 'NOT_ENOUGH_INFO':\n",
        "                nei_claims.append(claim)\n",
        "\n",
        "        print(f\"  Claims with evidence: {len(claims_with_evidence)}\")\n",
        "        print(f\"  NOT_ENOUGH_INFO claims: {len(nei_claims)}\")\n",
        "\n",
        "        # Process claims with evidence (positive examples)\n",
        "        for claim in claims_with_evidence:\n",
        "            for doc_id in claim.cited_doc_ids:\n",
        "                doc_int = int(doc_id)\n",
        "                if doc_int not in corpus:\n",
        "                    continue\n",
        "                doc = corpus[doc_int]\n",
        "                for sent_idx, sent in enumerate(doc.abstract):\n",
        "                    # Ground truth: is this sentence evidence for claim?\n",
        "                    is_evidence = 0\n",
        "                    evidence_label = 'NOT_ENOUGH_INFO'\n",
        "                    if claim.evidence and str(doc.doc_id) in claim.evidence:\n",
        "                        for ev in claim.evidence[str(doc.doc_id)]:\n",
        "                            if sent_idx in ev.get('sentences', []):\n",
        "                                is_evidence = 1\n",
        "                                evidence_label = ev.get('label')\n",
        "                                break\n",
        "\n",
        "                    claim_label = self.label_map.get(claim.label, 2)\n",
        "\n",
        "                    self.examples.append({\n",
        "                        'claim_id': claim.id,\n",
        "                        'doc_id': doc.doc_id,\n",
        "                        'sent_idx': sent_idx,\n",
        "                        'claim': claim.claim,\n",
        "                        'sentence': sent,\n",
        "                        'is_evidence': is_evidence,\n",
        "                        'claim_label': claim_label,\n",
        "                        'evidence_label_str': evidence_label\n",
        "                    })\n",
        "\n",
        "        num_positive = len(self.examples)\n",
        "        print(f\"  Positive examples created: {num_positive}\")\n",
        "\n",
        "        # EXTENSION: Add hard negative examples (only in training mode)\n",
        "        if mode == 'train' and negative_ratio > 0:\n",
        "            num_negatives_needed = int(num_positive * negative_ratio)\n",
        "            print(f\"  Adding {num_negatives_needed} hard negative examples...\")\n",
        "\n",
        "            # Get all document IDs for random sampling\n",
        "            all_doc_ids = list(corpus.keys())\n",
        "\n",
        "            # Sample NOT_ENOUGH_INFO claims for negative examples\n",
        "            sampled_nei_claims = random.sample(nei_claims, min(len(nei_claims), num_negatives_needed))\n",
        "\n",
        "            for claim in sampled_nei_claims:\n",
        "                # Pair with a random document (not from cited_doc_ids)\n",
        "                # Try to find a document that's NOT in cited_doc_ids\n",
        "                available_docs = [d for d in all_doc_ids if d not in [int(x) for x in claim.cited_doc_ids]]\n",
        "                if not available_docs:\n",
        "                    available_docs = all_doc_ids  # Fallback to all docs\n",
        "\n",
        "                random_doc_id = random.choice(available_docs)\n",
        "                doc = corpus[random_doc_id]\n",
        "\n",
        "                # Sample 1-3 random sentences from this document\n",
        "                num_sents_to_sample = min(random.randint(1, 3), len(doc.abstract))\n",
        "                sampled_sent_indices = random.sample(range(len(doc.abstract)), num_sents_to_sample)\n",
        "\n",
        "                for sent_idx in sampled_sent_indices:\n",
        "                    sent = doc.abstract[sent_idx]\n",
        "\n",
        "                    # This is a negative example: is_evidence = 0\n",
        "                    self.examples.append({\n",
        "                        'claim_id': claim.id,\n",
        "                        'doc_id': doc.doc_id,\n",
        "                        'sent_idx': sent_idx,\n",
        "                        'claim': claim.claim,\n",
        "                        'sentence': sent,\n",
        "                        'is_evidence': 0,  # Hard negative: no evidence\n",
        "                        'claim_label': 2,  # NOT_ENOUGH_INFO\n",
        "                        'evidence_label_str': 'NOT_ENOUGH_INFO'\n",
        "                    })\n",
        "\n",
        "            print(f\"  Total examples (pos + neg): {len(self.examples)}\")\n",
        "            print(f\"  Negative examples added: {len(self.examples) - num_positive}\")\n",
        "\n",
        "        # Print statistics\n",
        "        evidence_counts = Counter([ex['is_evidence'] for ex in self.examples])\n",
        "        print(f\"  Evidence distribution: {dict(evidence_counts)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        # Tokenize as pair: [CLS] claim [SEP] sentence [SEP]\n",
        "        encoding = self.tokenizer(\n",
        "            ex['claim'], ex['sentence'],\n",
        "            truncation='only_second',  # Truncate sentence, not claim\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item['is_evidence'] = torch.tensor(ex['is_evidence'], dtype=torch.float)\n",
        "        item['claim_label'] = torch.tensor(ex['claim_label'], dtype=torch.long)\n",
        "        item['claim_id'] = ex['claim_id']\n",
        "        item['doc_id'] = ex['doc_id']\n",
        "        item['sent_idx'] = ex['sent_idx']\n",
        "        return item\n",
        "\n",
        "print(\"Dataset class defined with hard negative mining!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671,
          "referenced_widgets": [
            "6b396ec19d6b4edfb4992135024ec995",
            "03fea3f1d8c443108cc96f85eead48ba",
            "99104e2ec46a445f997ff1a99114e1c3",
            "0f28afc844754bcd800a1b13b74cbd24",
            "4fdb08ba00a04633ae997cd518083bfc",
            "40a243517af34594845c9c28599a4656",
            "611315a4669a44a2a5cecf1745e8a063",
            "a559b524667f4af6bc5a91e2dce66f66",
            "137cfe7e16d543678ba525b530b307d3",
            "1d8fb6d407f2495391e17df41b7fd1f6",
            "edd6719253e14bd89767e97b7f93336a",
            "9e49145dfaed4b628fdf40002347b861",
            "e99213520445451b80af9f13f7f9ecb7",
            "6b968f3091284954a8fb8cff536e660f",
            "f874de4decff4263a54474c050e88b87",
            "f320329de4c140b289c0ce7cab268fab",
            "1528283d9c934555bd0bbeb815345732",
            "48ae9a163b284b9494480cd2f08c5a50",
            "68a233a7d0494b3899d1af0695ecc5e7",
            "8a048a6b74454456a56b66c452996fbd",
            "d05cd2a8a28a4c169df43ab50104dc60",
            "b7e2ada9f6bf4a6393b3de2e9a69844c",
            "9403910d265c427b9ebd6429725bbc01",
            "dcec6dbd796f44b9b0e6c2249ad971d9",
            "a9d51a513aa04c4998bdfd2cff955e4f",
            "c745e9c1079144bbb51698d681060109",
            "40c12ce39b724e08b28e71bfcea3995d",
            "fac48f49fc0f483c9bf78ac9754feecd",
            "0732adb0062c4b4cb4668b06025f920b",
            "b14a4c51d34e43d881a6a043c5e6a0e7",
            "f4a4c4804ac641cd9d8ce7a1c6a19b4e",
            "b79a202ac77a4d3ab0ffeac7733c0eff",
            "5f330fcf15ab49b39199d12c8a917e2b"
          ]
        },
        "id": "8-eH9XcxDIFm",
        "outputId": "3f328646-96ab-429f-de9e-7383f8f840cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b396ec19d6b4edfb4992135024ec995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e49145dfaed4b628fdf40002347b861",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9403910d265c427b9ebd6429725bbc01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating training dataset with hard negatives...\n",
            "  Claims with evidence: 505\n",
            "  NOT_ENOUGH_INFO claims: 304\n",
            "  Total examples: 9801\n",
            "    Positive (evidence=1): 1025\n",
            "    Local negatives (non-evidence in gold docs): 4660\n",
            "    NEI negatives (from NEI claims with cited docs): 2741\n",
            "    Hard negatives (lexical similarity): 1375\n",
            "  Evidence distribution: {0: 8776, 1: 1025}\n",
            "\n",
            "Creating dev dataset (no negatives)...\n",
            "  Claims with evidence: 188\n",
            "  NOT_ENOUGH_INFO claims: 112\n",
            "  Total examples: 3121\n",
            "    Positive (evidence=1): 366\n",
            "    Local negatives (non-evidence in gold docs): 1789\n",
            "    NEI negatives (from NEI claims with cited docs): 966\n",
            "    Hard negatives (lexical similarity): 0\n",
            "  Evidence distribution: {0: 2755, 1: 366}\n",
            "\n",
            "Data loaders created:\n",
            "  Train batches: 613\n",
            "  Dev batches: 196\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and create datasets\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"\\nCreating training dataset with hard negatives...\")\n",
        "train_dataset = ImprovedSciFactSentencePairDataset(\n",
        "    train_claims, corpus, tokenizer, max_len=MAX_LEN, mode='train',\n",
        "    use_lexical_hard_negatives=USE_LEXICAL_HARD_NEGATIVES,\n",
        "    hard_negative_ratio=HARD_NEGATIVE_RATIO,\n",
        "    max_hard_negatives_per_claim=MAX_HARD_NEGATIVES_PER_CLAIM\n",
        ")\n",
        "print(\"\\nCreating dev dataset (no negatives)...\")\n",
        "\n",
        "dev_dataset = ImprovedSciFactSentencePairDataset(\n",
        "    dev_claims, corpus, tokenizer, max_len=MAX_LEN, mode='dev',\n",
        "    use_lexical_hard_negatives=False  # No hard negatives in dev\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nData loaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Dev batches: {len(dev_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4186dc04e1014735a90a2d37c0a0f0e0",
            "36e733d00baf48868a310ca057ffee6f",
            "c7877fefb0aa4d02b9c495c18ee30a87",
            "1ca47477702a4d4ab7336cb244cd13a1",
            "f617d338e27d45ba85ce195a0f48cdf6",
            "beb9596b7a2645e99e56b5906dbcb224",
            "2e18ca2129ef417793c0ee9633acff8d",
            "4e770b1a7f3a4a18a07135e779529264",
            "fb5a1bb3f0434432a8f0c5c05cd7bf74",
            "9a9deb47ac924fc48b058eff3cc4b146",
            "c9fd808b53584b73829e4eb0be938773"
          ]
        },
        "id": "OFX5einrDIFn",
        "outputId": "961881ea-75a8-4123-80df-bc5e0c68cd58"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4186dc04e1014735a90a2d37c0a0f0e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model instantiated. Hidden size: 768\n",
            "Total parameters: 109,485,316\n"
          ]
        }
      ],
      "source": [
        "# Model: PubMedBERT encoder + two heads (same as original)\n",
        "class PubMedBERT_SciFact(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        # Evidence classifier (binary) on pooled output\n",
        "        self.evidence_head = nn.Linear(hidden, 1)\n",
        "        # Claim-level classifier (3-way) -- aggregated per claim\n",
        "        self.claim_classifier = nn.Linear(hidden, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "        evidence_logit = self.evidence_head(pooled).squeeze(-1)  # [batch]\n",
        "        claim_logit = self.claim_classifier(pooled)  # [batch, 3]\n",
        "        return evidence_logit, claim_logit\n",
        "\n",
        "model = PubMedBERT_SciFact(MODEL_NAME).to(DEVICE)\n",
        "print(f\"Model instantiated. Hidden size: {model.encoder.config.hidden_size}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSaxfY8pDIFn",
        "outputId": "459d069d-77d0-4b80-eda7-e4ea08ca6cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer configured\n"
          ]
        }
      ],
      "source": [
        "# Loss functions and optimizer\n",
        "# Loss functions\n",
        "focal_loss_evidence = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA).to(DEVICE)\n",
        "loss_fn_claim = nn.CrossEntropyLoss()\n",
        "loss_fn_claim = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Optimizer configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id7JlKsYDIFo",
        "outputId": "84e3cd68-cb82-463b-fe5c-14a124e76c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training function defined\n"
          ]
        }
      ],
      "source": [
        "# Training loop with hard negative examples\n",
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_ev_loss = 0.0\n",
        "    running_claim_loss = 0.0\n",
        "\n",
        "    evidence_correct = 0\n",
        "    evidence_total = 0\n",
        "    claim_correct = 0\n",
        "    claim_total = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc='Train'):\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        evidence_labels = batch['is_evidence'].to(DEVICE)\n",
        "        claim_labels = batch['claim_label'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        ev_logits, claim_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Evidence loss (per sentence)\n",
        "        # Evidence loss: Use Focal Loss\n",
        "        loss_ev = focal_loss_evidence(ev_logits, evidence_labels)\n",
        "\n",
        "        # Claim loss: aggregate sentence-level logits per claim\n",
        "        claim_ids = batch['claim_id']\n",
        "        claim_to_idxs = defaultdict(list)\n",
        "        for i, cid in enumerate(claim_ids):\n",
        "            claim_to_idxs[cid].append(i)\n",
        "\n",
        "        # Average claim_logits across sentences for same claim\n",
        "        agg_claim_logits = []\n",
        "        agg_claim_labels = []\n",
        "        for cid, idxs in claim_to_idxs.items():\n",
        "            logits_avg = claim_logits[idxs].mean(dim=0, keepdim=True)  # [1,3]\n",
        "            agg_claim_logits.append(logits_avg)\n",
        "            agg_claim_labels.append(claim_labels[idxs[0]])\n",
        "\n",
        "        if agg_claim_logits:\n",
        "            agg_claim_logits = torch.cat(agg_claim_logits, dim=0).to(DEVICE)\n",
        "            agg_claim_labels = torch.tensor(agg_claim_labels, dtype=torch.long).to(DEVICE)\n",
        "            loss_claim = loss_fn_claim(agg_claim_logits, agg_claim_labels)\n",
        "        else:\n",
        "            loss_claim = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        # Combined loss (FIXED: weight evidence loss, not claim loss)\n",
        "        total_loss = loss_claim + EVIDENCE_LOSS_WEIGHT * loss_ev\n",
        "\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += total_loss.item()\n",
        "        running_ev_loss += loss_ev.item()\n",
        "        running_claim_loss += loss_claim.item()\n",
        "\n",
        "        # Evidence accuracy\n",
        "        ev_preds = (torch.sigmoid(ev_logits) > 0.5).float()\n",
        "        evidence_correct += (ev_preds == evidence_labels).sum().item()\n",
        "        evidence_total += evidence_labels.size(0)\n",
        "\n",
        "        # Claim accuracy\n",
        "        if agg_claim_logits is not None and len(agg_claim_logits) > 0:\n",
        "            claim_preds = agg_claim_logits.argmax(dim=1)\n",
        "            claim_correct += (claim_preds == agg_claim_labels).sum().item()\n",
        "            claim_total += len(agg_claim_labels)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_ev_loss = running_ev_loss / len(train_loader)\n",
        "    avg_claim_loss = running_claim_loss / len(train_loader)\n",
        "    ev_acc = evidence_correct / evidence_total if evidence_total > 0 else 0\n",
        "    claim_acc = claim_correct / claim_total if claim_total > 0 else 0\n",
        "\n",
        "    return avg_loss, avg_ev_loss, avg_claim_loss, ev_acc, claim_acc\n",
        "\n",
        "print(\"Training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e12W8tnNOWF8"
      },
      "outputs": [],
      "source": [
        "NEGATIVE_RATIO=0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkQFvLRNDIFo",
        "outputId": "a8a177d9-05d5-441e-d9e2-e39db55986b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING WITH HARD NEGATIVE MINING\n",
            "Negative ratio: 0.3\n",
            "Epochs: 6\n",
            "============================================================\n",
            "\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:15<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.6132 (ev: 0.0331, claim: 0.5469)\n",
            "  Evidence Acc: 0.8639\n",
            "  Claim Acc: 0.7570\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch1.pt\n",
            "\n",
            "Epoch 2/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:16<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.1627 (ev: 0.0260, claim: 0.1108)\n",
            "  Evidence Acc: 0.8958\n",
            "  Claim Acc: 0.9650\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch2.pt\n",
            "\n",
            "Epoch 3/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:16<00:00,  1.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.0868 (ev: 0.0206, claim: 0.0456)\n",
            "  Evidence Acc: 0.9258\n",
            "  Claim Acc: 0.9870\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch3.pt\n",
            "\n",
            "Epoch 4/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:16<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.0638 (ev: 0.0160, claim: 0.0318)\n",
            "  Evidence Acc: 0.9496\n",
            "  Claim Acc: 0.9926\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch4.pt\n",
            "\n",
            "Epoch 5/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:16<00:00,  1.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.0412 (ev: 0.0106, claim: 0.0200)\n",
            "  Evidence Acc: 0.9705\n",
            "  Claim Acc: 0.9944\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch5.pt\n",
            "\n",
            "Epoch 6/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: 100%|██████████| 613/613 [07:16<00:00,  1.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Loss: 0.0258 (ev: 0.0066, claim: 0.0127)\n",
            "  Evidence Acc: 0.9833\n",
            "  Claim Acc: 0.9961\n",
            "  Saved: models/claim_verifier/pubmedbert_ext1_epoch6.pt\n",
            "\n",
            "Training complete! Best loss: 0.0258\n",
            "Best checkpoint: models/claim_verifier/pubmedbert_ext1_epoch6.pt\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "print(\"TRAINING WITH HARD NEGATIVE MINING\")\n",
        "print(f\"Negative ratio: {NEGATIVE_RATIO}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_loss = float('inf')\n",
        "training_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    avg_loss, avg_ev_loss, avg_claim_loss, ev_acc, claim_acc = train_one_epoch()\n",
        "\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': avg_loss,\n",
        "        'ev_loss': avg_ev_loss,\n",
        "        'claim_loss': avg_claim_loss,\n",
        "        'ev_acc': ev_acc,\n",
        "        'claim_acc': claim_acc\n",
        "    })\n",
        "\n",
        "    print(f\"  Loss: {avg_loss:.4f} (ev: {avg_ev_loss:.4f}, claim: {avg_claim_loss:.4f})\")\n",
        "    print(f\"  Evidence Acc: {ev_acc:.4f}\")\n",
        "    print(f\"  Claim Acc: {claim_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = f'models/claim_verifier/pubmedbert_ext1_epoch{epoch+1}.pt'\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"  Saved: {checkpoint_path}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_checkpoint = checkpoint_path\n",
        "\n",
        "print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
        "print(f\"Best checkpoint: {best_checkpoint}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ_Z7civDIFo",
        "outputId": "94387d4d-dd26-4b60-e288-ad0faa058d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction function defined\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions for evaluation\n",
        "def generate_predictions(model, claims, corpus, tokenizer, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Generate predictions in SciFact format.\n",
        "    Uses oracle retrieval (gold documents) for now.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for claim in tqdm(claims, desc=\"Generating predictions\"):\n",
        "            if not claim.cited_doc_ids:\n",
        "                predictions.append({\n",
        "                    'id': claim.id,\n",
        "                    'label': 'NOT_ENOUGH_INFO',\n",
        "                    'evidence': {}\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Use oracle retrieval (first cited doc)\n",
        "            doc_id = int(claim.cited_doc_ids[0])\n",
        "            if doc_id not in corpus:\n",
        "                predictions.append({\n",
        "                    'id': claim.id,\n",
        "                    'label': 'NOT_ENOUGH_INFO',\n",
        "                    'evidence': {}\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            doc = corpus[doc_id]\n",
        "            claim_evidence = {}\n",
        "\n",
        "            # Process each sentence in the document\n",
        "            sentence_scores = []\n",
        "            for sent_idx, sent in enumerate(doc.abstract):\n",
        "                # Tokenize claim-sentence pair\n",
        "                encoding = tokenizer(\n",
        "                    claim.claim, sent,\n",
        "                    truncation='only_second',\n",
        "                    padding='max_length',\n",
        "                    max_length=MAX_LEN,\n",
        "                    return_tensors='pt'\n",
        "                ).to(device)\n",
        "\n",
        "                # Get predictions\n",
        "                ev_logits, claim_logits = model(encoding['input_ids'], encoding['attention_mask'])\n",
        "                ev_prob = torch.sigmoid(ev_logits).item()\n",
        "\n",
        "                sentence_scores.append({\n",
        "                    'sent_idx': sent_idx,\n",
        "                    'ev_prob': ev_prob,\n",
        "                    'claim_logits': claim_logits[0].cpu().numpy()\n",
        "                })\n",
        "\n",
        "            # Aggregate claim-level prediction (average logits)\n",
        "            if sentence_scores:\n",
        "                avg_claim_logits = np.mean([s['claim_logits'] for s in sentence_scores], axis=0)\n",
        "                pred_label_idx = np.argmax(avg_claim_logits)\n",
        "                label_map = {0: 'SUPPORT', 1: 'CONTRADICT', 2: 'NOT_ENOUGH_INFO'}\n",
        "                pred_label = label_map[pred_label_idx]\n",
        "            else:\n",
        "                pred_label = 'NOT_ENOUGH_INFO'\n",
        "\n",
        "            # Select evidence sentences above threshold\n",
        "            pred_evidence_sents = [\n",
        "                s['sent_idx'] for s in sentence_scores\n",
        "                if s['ev_prob'] > threshold\n",
        "            ]\n",
        "\n",
        "            # Build prediction\n",
        "            prediction = {\n",
        "                'id': claim.id,\n",
        "                'label': pred_label,\n",
        "                'evidence': {}\n",
        "            }\n",
        "\n",
        "            # Dont force NEI - use stance classifier's prediction\n",
        "            if pred_evidence_sents:\n",
        "                prediction['evidence'][str(doc_id)] = [{\n",
        "                    'sentences': pred_evidence_sents,\n",
        "                    'label': pred_label\n",
        "                }]\n",
        "            # If no evidence but classifier says SUPPORT/CONTRADICT, still use that label\n",
        "            # (pred_label already set from classifier above)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"Prediction function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238K4uk9DIFp",
        "outputId": "fa105db0-7f7a-4772-e814-6c64f5caaf7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION: Testing Different Thresholds\n",
            "\n",
            "--- Threshold: 0.3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:50<00:00,  5.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.4749\n",
            "  F1:        0.6440\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.2855\n",
            "  Recall:    0.4290\n",
            "  F1:        0.3428\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Threshold: 0.4 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:48<00:00,  6.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.4484\n",
            "  F1:        0.6191\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.2970\n",
            "  Recall:    0.4016\n",
            "  F1:        0.3415\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Threshold: 0.5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:48<00:00,  6.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.4248\n",
            "  F1:        0.5963\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.3119\n",
            "  Recall:    0.3716\n",
            "  F1:        0.3392\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Threshold: 0.55 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:48<00:00,  6.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.4130\n",
            "  F1:        0.5846\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.3090\n",
            "  Recall:    0.3579\n",
            "  F1:        0.3316\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Threshold: 0.6 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:49<00:00,  6.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.4071\n",
            "  F1:        0.5786\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.3098\n",
            "  Recall:    0.3470\n",
            "  F1:        0.3273\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Threshold: 0.7 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:49<00:00,  6.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.3658\n",
            "  F1:        0.5356\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.3175\n",
            "  Recall:    0.3115\n",
            "  F1:        0.3145\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is competitive\n",
            "============================================================\n",
            "\n",
            "Best Threshold: 0.5 (F1: 0.0000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on dev set with different thresholds\n",
        "print(\"EVALUATION: Testing Different Thresholds\")\n",
        "\n",
        "best_f1 = 0\n",
        "best_threshold = 0.5\n",
        "results = []\n",
        "\n",
        "for threshold in [0.3, 0.4, 0.5, 0.55, 0.6, 0.7]:\n",
        "    print(f\"\\n--- Threshold: {threshold} ---\")\n",
        "\n",
        "    predictions = generate_predictions(model, dev_claims, corpus, tokenizer, DEVICE, threshold=threshold)\n",
        "\n",
        "    # Save predictions\n",
        "    output_path = f'output/dev/pubmedbert_ext1_thresh{int(threshold*100)}.jsonl'\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with jsonlines.open(output_path, 'w') as writer:\n",
        "        writer.write_all(predictions)\n",
        "\n",
        "    # Evaluate using the scoring script\n",
        "    import subprocess\n",
        "    result = subprocess.run(\n",
        "        ['python', 'src/evaluation/score_claims.py',\n",
        "         '--gold', 'data/scifact/data/claims_dev.jsonl',\n",
        "         '--predictions', output_path],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    print(result.stdout)\n",
        "\n",
        "    # Parse F1 from output (simple extraction)\n",
        "    if 'Sentence-level' in result.stdout:\n",
        "        lines = result.stdout.split('\\n')\n",
        "        # Find the line with \"Sentence-level\" and look for F1 in subsequent lines\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'Sentence-level' in line:\n",
        "                # Look for F1 in the next 5 lines\n",
        "                for j in range(i+1, min(i+6, len(lines))):\n",
        "                    if 'F1:' in lines[j]:\n",
        "                        try:\n",
        "                            f1 = float(lines[j].split('F1:')[1].strip().split()[0])\n",
        "                            results.append({'threshold': threshold, 'f1': f1})\n",
        "                            if f1 > best_f1:\n",
        "                                best_f1 = f1\n",
        "                                best_threshold = threshold\n",
        "                            break\n",
        "                        except:\n",
        "                            pass\n",
        "                break\n",
        "\n",
        "print(f\"Best Threshold: {best_threshold} (F1: {best_f1:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzQL-XLrDIFp"
      },
      "source": [
        "## Results Summary\n",
        "\n",
        "Compare with baseline:\n",
        "- **Baseline (PubMedBERT)**: 39.30% F1\n",
        "- **With Hard Negatives + Focal Loss**: 34.28% F1 (best at threshold 0.3)\n",
        "\n",
        "### Analysis\n",
        "\n",
        "This extension did not improve upon the baseline. While Focal Loss and better negative mining improved NEI classification, they made the evidence head more conservative, reducing recall on evidence sentences. The combination of increased NEI training examples and Focal Loss's focus on hard examples led the model to be overly cautious about predicting evidence, which hurt sentence-level F1. This demonstrates that the baseline's simpler approach (standard BCE with all claims) was already well-tuned for the sentence-level F1 metric, which primarily rewards correct evidence extraction for SUPPORT/CONTRADICT claims rather than NEI classification accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03fea3f1d8c443108cc96f85eead48ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40a243517af34594845c9c28599a4656",
            "placeholder": "​",
            "style": "IPY_MODEL_611315a4669a44a2a5cecf1745e8a063",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0732adb0062c4b4cb4668b06025f920b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f28afc844754bcd800a1b13b74cbd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8fb6d407f2495391e17df41b7fd1f6",
            "placeholder": "​",
            "style": "IPY_MODEL_edd6719253e14bd89767e97b7f93336a",
            "value": " 28.0/28.0 [00:00&lt;00:00, 3.35kB/s]"
          }
        },
        "137cfe7e16d543678ba525b530b307d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1528283d9c934555bd0bbeb815345732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca47477702a4d4ab7336cb244cd13a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a9deb47ac924fc48b058eff3cc4b146",
            "placeholder": "​",
            "style": "IPY_MODEL_c9fd808b53584b73829e4eb0be938773",
            "value": " 440M/440M [00:07&lt;00:00, 52.7MB/s]"
          }
        },
        "1d8fb6d407f2495391e17df41b7fd1f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e18ca2129ef417793c0ee9633acff8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36e733d00baf48868a310ca057ffee6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb9596b7a2645e99e56b5906dbcb224",
            "placeholder": "​",
            "style": "IPY_MODEL_2e18ca2129ef417793c0ee9633acff8d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "40a243517af34594845c9c28599a4656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c12ce39b724e08b28e71bfcea3995d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4186dc04e1014735a90a2d37c0a0f0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36e733d00baf48868a310ca057ffee6f",
              "IPY_MODEL_c7877fefb0aa4d02b9c495c18ee30a87",
              "IPY_MODEL_1ca47477702a4d4ab7336cb244cd13a1"
            ],
            "layout": "IPY_MODEL_f617d338e27d45ba85ce195a0f48cdf6"
          }
        },
        "48ae9a163b284b9494480cd2f08c5a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e770b1a7f3a4a18a07135e779529264": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fdb08ba00a04633ae997cd518083bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f330fcf15ab49b39199d12c8a917e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "611315a4669a44a2a5cecf1745e8a063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a233a7d0494b3899d1af0695ecc5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b396ec19d6b4edfb4992135024ec995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03fea3f1d8c443108cc96f85eead48ba",
              "IPY_MODEL_99104e2ec46a445f997ff1a99114e1c3",
              "IPY_MODEL_0f28afc844754bcd800a1b13b74cbd24"
            ],
            "layout": "IPY_MODEL_4fdb08ba00a04633ae997cd518083bfc"
          }
        },
        "6b968f3091284954a8fb8cff536e660f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a233a7d0494b3899d1af0695ecc5e7",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a048a6b74454456a56b66c452996fbd",
            "value": 385
          }
        },
        "8a048a6b74454456a56b66c452996fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9403910d265c427b9ebd6429725bbc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcec6dbd796f44b9b0e6c2249ad971d9",
              "IPY_MODEL_a9d51a513aa04c4998bdfd2cff955e4f",
              "IPY_MODEL_c745e9c1079144bbb51698d681060109"
            ],
            "layout": "IPY_MODEL_40c12ce39b724e08b28e71bfcea3995d"
          }
        },
        "99104e2ec46a445f997ff1a99114e1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a559b524667f4af6bc5a91e2dce66f66",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_137cfe7e16d543678ba525b530b307d3",
            "value": 28
          }
        },
        "9a9deb47ac924fc48b058eff3cc4b146": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e49145dfaed4b628fdf40002347b861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e99213520445451b80af9f13f7f9ecb7",
              "IPY_MODEL_6b968f3091284954a8fb8cff536e660f",
              "IPY_MODEL_f874de4decff4263a54474c050e88b87"
            ],
            "layout": "IPY_MODEL_f320329de4c140b289c0ce7cab268fab"
          }
        },
        "a559b524667f4af6bc5a91e2dce66f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d51a513aa04c4998bdfd2cff955e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b14a4c51d34e43d881a6a043c5e6a0e7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4a4c4804ac641cd9d8ce7a1c6a19b4e",
            "value": 1
          }
        },
        "b14a4c51d34e43d881a6a043c5e6a0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b79a202ac77a4d3ab0ffeac7733c0eff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e2ada9f6bf4a6393b3de2e9a69844c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb9596b7a2645e99e56b5906dbcb224": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c745e9c1079144bbb51698d681060109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79a202ac77a4d3ab0ffeac7733c0eff",
            "placeholder": "​",
            "style": "IPY_MODEL_5f330fcf15ab49b39199d12c8a917e2b",
            "value": " 226k/? [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "c7877fefb0aa4d02b9c495c18ee30a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e770b1a7f3a4a18a07135e779529264",
            "max": 440474434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb5a1bb3f0434432a8f0c5c05cd7bf74",
            "value": 440474434
          }
        },
        "c9fd808b53584b73829e4eb0be938773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05cd2a8a28a4c169df43ab50104dc60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcec6dbd796f44b9b0e6c2249ad971d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fac48f49fc0f483c9bf78ac9754feecd",
            "placeholder": "​",
            "style": "IPY_MODEL_0732adb0062c4b4cb4668b06025f920b",
            "value": "vocab.txt: "
          }
        },
        "e99213520445451b80af9f13f7f9ecb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1528283d9c934555bd0bbeb815345732",
            "placeholder": "​",
            "style": "IPY_MODEL_48ae9a163b284b9494480cd2f08c5a50",
            "value": "config.json: 100%"
          }
        },
        "edd6719253e14bd89767e97b7f93336a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f320329de4c140b289c0ce7cab268fab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a4c4804ac641cd9d8ce7a1c6a19b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f617d338e27d45ba85ce195a0f48cdf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f874de4decff4263a54474c050e88b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d05cd2a8a28a4c169df43ab50104dc60",
            "placeholder": "​",
            "style": "IPY_MODEL_b7e2ada9f6bf4a6393b3de2e9a69844c",
            "value": " 385/385 [00:00&lt;00:00, 47.6kB/s]"
          }
        },
        "fac48f49fc0f483c9bf78ac9754feecd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5a1bb3f0434432a8f0c5c05cd7bf74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
