{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNN_TITLE"
      },
      "source": [
        "# Extension: Graph Neural Network for Claim Verification\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a Graph Neural Network (GNN) extension to improve claim verification by modeling relationships between claims and document sentences.\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Current models (SciBERT, PubMedBERT) process sentences independently, missing:\n",
        "- **Claim-sentence relationships**: Which sentences directly address the claim?\n",
        "- **Sentence-sentence dependencies**: How do sentences relate to each other?\n",
        "- **Document structure**: Sequential and semantic relationships\n",
        "\n",
        "GNNs can explicitly model these relationships through graph attention, potentially improving evidence extraction.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "**Graph Construction**:\n",
        "- **Nodes**: 1 claim node + N sentence nodes (one per sentence)\n",
        "- **Edges**:\n",
        "  - Claim ↔ Sentence (all sentences, bidirectional, weighted by cosine similarity)\n",
        "  - Sentence ↔ Sentence (sequential: adjacent + semantic: similarity > threshold)\n",
        "\n",
        "**Model**:\n",
        "- BERT (SciBERT/PubMedBERT) encodes claim and sentences separately **WITH GRADIENTS** (trainable)\n",
        "- GAT (Graph Attention Network) refines representations\n",
        "- Hybrid: Concatenate BERT + GNN embeddings\n",
        "- Classifiers: Label (3-way) + Evidence (binary per sentence)\n",
        "\n",
        "### Hope to see  - Impact\n",
        "\n",
        "- **Target**: +3-5% F1 improvement over baseline (with fixes applied)\n",
        "- **Mechanism**: Fine-tuned BERT + relational modeling through GNN\n",
        "- **Best Case**: +5-7% F1 if GNN captures important relationships\n",
        "\n",
        "### Baseline Comparison\n",
        "\n",
        "| Model | F1 | Notes |\n",
        "|-------|-----|-------|\n",
        "| SciBERT Baseline | 24.20% | Milestone 2 |\n",
        "| PubMedBERT | 39.30% | Sentence-pair architecture |\n",
        "| **SciBERT + GNN** | 18.98% | Frozen BERT, no NEI training |\n",
        "| **SciBERT + GNN** | ? | Trainable BERT, with NEI training |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SETUP_DRIVE",
        "outputId": "f16d04a1-3c1b-40d0-d5e7-d48d701de6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Setup: Mount Google Drive and install dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import torch\n",
        "import os\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "INSTALL_DEPS"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q transformers datasets jsonlines scikit-learn tqdm\n",
        "%pip install -q torch-geometric  # GNN library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLONE_REPO",
        "outputId": "bf1c1f20-92c6-4306-eeb4-3f76b2d1b780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cis5300_project'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 286 (delta 160), reused 97 (delta 30), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (286/286), 14.30 MiB | 12.86 MiB/s, done.\n",
            "Resolving deltas: 100% (160/160), done.\n",
            "Current directory: /content/cis5300_project\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "!rm -rf cis5300_project\n",
        "!git clone https://github.com/asxd-10/cis5300_project.git\n",
        "\n",
        "import sys\n",
        "os.chdir('cis5300_project')\n",
        "sys.path.append('.')\n",
        "print(f\"Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CONFIG",
        "outputId": "3dd1bc53-ed9c-4a72-95e8-b6b974946960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Model: allenai/scibert_scivocab_uncased\n",
            "GNN: 2 layers, 4 heads, hidden_dim=256\n",
            "Hybrid mode: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = 'allenai/scibert_scivocab_uncased'  # or 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "USE_PUBMEDBERT = False  # Set to True to use PubMedBERT\n",
        "\n",
        "if USE_PUBMEDBERT:\n",
        "    MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "\n",
        "# GNN configuration\n",
        "GNN_HIDDEN_DIM = 256  # GNN hidden dimension (can tune: 128, 256, 512)\n",
        "GNN_NUM_LAYERS = 2  # Number of GAT layers (can tune: 1, 2, 3)\n",
        "GNN_NUM_HEADS = 4  # Attention heads per layer (can tune: 2, 4, 8)\n",
        "GNN_DROPOUT = 0.1\n",
        "SIMILARITY_THRESHOLD = 0.3  # Min similarity for sentence-sentence edges\n",
        "\n",
        "# Training configuration\n",
        "BATCH_SIZE = 4  # Smaller batch for graphs (can increase if memory allows)\n",
        "LEARNING_RATE = 1e-4  # Lower LR for GNN (can tune: 5e-5, 1e-4, 2e-4)\n",
        "EPOCHS = 6\n",
        "MAX_SENTENCES = 20  # Max sentences per document\n",
        "EVIDENCE_LOSS_WEIGHT = 2.0\n",
        "NEGATIVE_RATIO = 0.3  # Add 30% NEI examples for training\n",
        "\n",
        "# Hybrid configuration\n",
        "USE_HYBRID = True  # Concatenate BERT + GNN embeddings (recommended)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"GNN: {GNN_NUM_LAYERS} layers, {GNN_NUM_HEADS} heads, hidden_dim={GNN_HIDDEN_DIM}\")\n",
        "print(f\"Hybrid mode: {USE_HYBRID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOAD_DATA",
        "outputId": "ab893506-b2da-4e45-8c8e-ccbd52fcc0c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "809 training claims\n",
            "300 dev claims\n",
            "5183 documents\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "from src.common.data_utils import load_claims, load_corpus\n",
        "\n",
        "train_claims = load_claims('data/scifact/data/claims_train.jsonl')\n",
        "dev_claims = load_claims('data/scifact/data/claims_dev.jsonl')\n",
        "corpus = load_corpus('data/scifact/data/corpus.jsonl')\n",
        "\n",
        "print(f\"{len(train_claims)} training claims\")\n",
        "print(f\"{len(dev_claims)} dev claims\")\n",
        "print(f\"{len(corpus)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRAPH_UTILS",
        "outputId": "39a2ec19-2926-4015-a08b-7e3c0134d53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph construction utilities defined\n"
          ]
        }
      ],
      "source": [
        "# Graph Construction Utilities\n",
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cosine_similarity_torch(a, b):\n",
        "    \"\"\"Compute cosine similarity between two tensors.\"\"\"\n",
        "    a_norm = F.normalize(a, p=2, dim=-1)\n",
        "    b_norm = F.normalize(b, p=2, dim=-1)\n",
        "    return (a_norm * b_norm).sum(dim=-1)\n",
        "\n",
        "def build_claim_sentence_graph(claim_emb, sent_embs, similarity_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Build graph connecting claim and sentences.\n",
        "\n",
        "    Args:\n",
        "        claim_emb: [768] - claim embedding from BERT\n",
        "        sent_embs: [N, 768] - sentence embeddings from BERT\n",
        "        similarity_threshold: Min similarity for sentence-sentence edges\n",
        "\n",
        "    Returns:\n",
        "        edge_index: [2, E] - edge connections\n",
        "        edge_weights: [E] - cosine similarities (optional)\n",
        "    \"\"\"\n",
        "    N = len(sent_embs)\n",
        "    if N == 0:\n",
        "        # Empty document - just claim node\n",
        "        return torch.empty((2, 0), dtype=torch.long), torch.empty((0,), dtype=torch.float)\n",
        "\n",
        "    edges = []\n",
        "    weights = []\n",
        "\n",
        "    # Claim-sentence edges (bidirectional)\n",
        "    # Node 0 = claim, Nodes 1..N = sentences\n",
        "    for i in range(N):\n",
        "        # Claim -> Sentence i\n",
        "        sim = cosine_similarity_torch(claim_emb.unsqueeze(0), sent_embs[i:i+1]).item()\n",
        "        edges.append([0, i+1])\n",
        "        weights.append(sim)\n",
        "\n",
        "        # Sentence i -> Claim (bidirectional)\n",
        "        edges.append([i+1, 0])\n",
        "        weights.append(sim)\n",
        "\n",
        "    # Sentence-sentence edges\n",
        "    # Sequential edges (adjacent sentences)\n",
        "    for i in range(N-1):\n",
        "        sim = cosine_similarity_torch(sent_embs[i:i+1], sent_embs[i+1:i+2]).item()\n",
        "        # i -> i+1\n",
        "        edges.append([i+1, i+2])\n",
        "        weights.append(sim)\n",
        "        # i+1 -> i (bidirectional)\n",
        "        edges.append([i+2, i+1])\n",
        "        weights.append(sim)\n",
        "\n",
        "    # Semantic edges (high similarity, not adjacent)\n",
        "    for i in range(N):\n",
        "        for j in range(i+2, N):  # Skip adjacent (already added)\n",
        "            sim = cosine_similarity_torch(sent_embs[i:i+1], sent_embs[j:j+1]).item()\n",
        "            if sim > similarity_threshold:\n",
        "                edges.append([i+1, j+1])\n",
        "                weights.append(sim)\n",
        "                edges.append([j+1, i+1])\n",
        "                weights.append(sim)\n",
        "\n",
        "    if len(edges) == 0:\n",
        "        # Fallback: at least connect claim to sentences\n",
        "        for i in range(N):\n",
        "            edges.append([0, i+1])\n",
        "            weights.append(1.0)\n",
        "\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    edge_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "    return edge_index, edge_weights\n",
        "\n",
        "print(\"Graph construction utilities defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccu2VgMFCCqm",
        "outputId": "91ecdae3-f273-4409-d8af-c9dabf0fcd28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN model defined\n"
          ]
        }
      ],
      "source": [
        "# GNN-Enhanced Model Architecture\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GNNClaimVerifier(nn.Module):\n",
        "    \"\"\"\n",
        "    GNN-enhanced claim verification model.\n",
        "    Combines BERT encoding with Graph Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='allenai/scibert_scivocab_uncased',\n",
        "                 num_labels=3, max_sentences=20,\n",
        "                 gnn_hidden_dim=256, gnn_num_layers=2, gnn_num_heads=4,\n",
        "                 gnn_dropout=0.1, use_hybrid=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # BERT encoder\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size  # 768 for SciBERT\n",
        "\n",
        "        # GNN layers\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        self.gnn_hidden_dim = gnn_hidden_dim\n",
        "        self.use_hybrid = use_hybrid\n",
        "\n",
        "        # First GAT layer\n",
        "        self.gnn_layers.append(\n",
        "            GATConv(hidden_size, gnn_hidden_dim, heads=gnn_num_heads,\n",
        "                   dropout=gnn_dropout, concat=True)\n",
        "        )\n",
        "\n",
        "        # Additional GAT layers\n",
        "        for _ in range(gnn_num_layers - 1):\n",
        "            self.gnn_layers.append(\n",
        "                GATConv(gnn_hidden_dim * gnn_num_heads, gnn_hidden_dim,\n",
        "                       heads=gnn_num_heads, dropout=gnn_dropout, concat=True)\n",
        "            )\n",
        "\n",
        "        # Final GNN output dimension\n",
        "        gnn_output_dim = gnn_hidden_dim * gnn_num_heads\n",
        "\n",
        "        # Classification heads\n",
        "        if use_hybrid:\n",
        "            # Hybrid: Concatenate BERT + GNN\n",
        "            classifier_input_dim = hidden_size + gnn_output_dim\n",
        "        else:\n",
        "            # GNN only\n",
        "            classifier_input_dim = gnn_output_dim\n",
        "\n",
        "        # Label classifier (uses claim node)\n",
        "        self.label_classifier = nn.Sequential(\n",
        "            nn.Linear(classifier_input_dim, classifier_input_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(classifier_input_dim // 2, num_labels)\n",
        "        )\n",
        "\n",
        "        # Evidence classifier (uses sentence nodes)\n",
        "        self.evidence_classifier = nn.Sequential(\n",
        "            nn.Linear(classifier_input_dim, classifier_input_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(classifier_input_dim // 2, 1)  # Binary\n",
        "        )\n",
        "\n",
        "        self.max_sentences = max_sentences\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gnn_output_dim = gnn_output_dim\n",
        "\n",
        "    def encode_claim_and_sentences(self, claim_text, sentences, device):\n",
        "        \"\"\"\n",
        "        Encode claim and sentences separately using BERT.\n",
        "\n",
        "        Returns:\n",
        "            claim_emb: [768] - claim embedding\n",
        "            sent_embs: [N, 768] - sentence embeddings\n",
        "        \"\"\"\n",
        "        # Encode claim\n",
        "        claim_encoding = self.tokenizer(\n",
        "            claim_text,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        claim_output = self.encoder(**claim_encoding)\n",
        "        claim_emb = claim_output.last_hidden_state[:, 0, :].squeeze(0)  # [CLS] token\n",
        "\n",
        "        # Encode sentences\n",
        "        sent_embs = []\n",
        "        for sent in sentences:\n",
        "            sent_encoding = self.tokenizer(\n",
        "                sent,\n",
        "                max_length=128,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            ).to(device)\n",
        "\n",
        "            sent_output = self.encoder(**sent_encoding)\n",
        "            sent_emb = sent_output.last_hidden_state[:, 0, :].squeeze(0)  # [CLS] token\n",
        "            sent_embs.append(sent_emb)\n",
        "\n",
        "        if len(sent_embs) > 0:\n",
        "            sent_embs = torch.stack(sent_embs)  # [N, 768]\n",
        "        else:\n",
        "            # Empty document\n",
        "            sent_embs = torch.zeros((0, claim_emb.size(0)), device=device)\n",
        "\n",
        "        return claim_emb, sent_embs\n",
        "\n",
        "    def forward(self, claim_tokens_list, sentence_tokens_lists, num_sentences_list, similarity_thresholds):\n",
        "        \"\"\"\n",
        "        Forward pass with tokenized inputs - BERT is now trainable.\n",
        "\n",
        "        Args:\n",
        "            claim_tokens_list: List of tokenized claims [batch_size]\n",
        "            sentence_tokens_lists: List of lists of tokenized sentences [batch_size, num_sents]\n",
        "            num_sentences_list: List of actual sentence counts per example\n",
        "            similarity_thresholds: List of similarity thresholds per example\n",
        "\n",
        "        Returns:\n",
        "            label_logits: [batch, 3]\n",
        "            evidence_logits: [batch, max_sentences]\n",
        "        \"\"\"\n",
        "        batch_size = len(claim_tokens_list)\n",
        "        all_graphs = []\n",
        "\n",
        "        # Encode each example and build graph (WITH gradients - BERT trainable)\n",
        "        for i in range(batch_size):\n",
        "            claim_tokens = claim_tokens_list[i].to(self.encoder.device)\n",
        "            sentence_tokens_list = sentence_tokens_lists[i]\n",
        "            num_sents = num_sentences_list[i]\n",
        "            sim_thresh = similarity_thresholds[i]\n",
        "\n",
        "            # Encode claim (WITH gradients - BERT trainable)\n",
        "            claim_output = self.encoder(**claim_tokens)\n",
        "            claim_emb = claim_output.last_hidden_state[:, 0, :].squeeze(0)  # [768]\n",
        "\n",
        "            # Encode sentences (WITH gradients)\n",
        "            sent_embs = []\n",
        "            for sent_tokens in sentence_tokens_list:\n",
        "                sent_tokens_device = sent_tokens.to(self.encoder.device)\n",
        "                sent_output = self.encoder(**sent_tokens_device)\n",
        "                sent_emb = sent_output.last_hidden_state[:, 0, :].squeeze(0)  # [768]\n",
        "                sent_embs.append(sent_emb)\n",
        "\n",
        "            if len(sent_embs) > 0:\n",
        "                sent_embs = torch.stack(sent_embs)  # [N, 768]\n",
        "            else:\n",
        "                sent_embs = torch.zeros((0, claim_emb.size(0)), device=claim_emb.device)\n",
        "\n",
        "            # Build graph\n",
        "            edge_index, edge_weights = build_claim_sentence_graph(\n",
        "                claim_emb, sent_embs, sim_thresh\n",
        "            )\n",
        "\n",
        "            # Create node features\n",
        "            node_features = torch.cat([claim_emb.unsqueeze(0), sent_embs], dim=0)\n",
        "\n",
        "            # Create graph\n",
        "            graph = Data(\n",
        "                x=node_features,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=edge_weights\n",
        "            )\n",
        "            all_graphs.append(graph)\n",
        "\n",
        "        # Batch graphs\n",
        "        batched_graphs = Batch.from_data_list(all_graphs).to(claim_emb.device)\n",
        "\n",
        "        # Process through GNN\n",
        "        x = batched_graphs.x\n",
        "        edge_index = batched_graphs.edge_index\n",
        "        bert_embeddings = x.clone()\n",
        "\n",
        "        # Apply GNN layers\n",
        "        for gnn_layer in self.gnn_layers:\n",
        "            x = gnn_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "\n",
        "        # Split and classify\n",
        "        batch_assignments = batched_graphs.batch\n",
        "        label_logits_list = []\n",
        "        evidence_logits_list = []\n",
        "\n",
        "        for graph_idx in range(batch_size):\n",
        "            graph_mask = (batch_assignments == graph_idx)\n",
        "            graph_nodes_gnn = x[graph_mask]\n",
        "            graph_nodes_bert = bert_embeddings[graph_mask]\n",
        "\n",
        "            claim_node_gnn = graph_nodes_gnn[0]\n",
        "            claim_node_bert = graph_nodes_bert[0]\n",
        "            sentence_nodes_gnn = graph_nodes_gnn[1:]\n",
        "            sentence_nodes_bert = graph_nodes_bert[1:]\n",
        "\n",
        "            # Pad sentences\n",
        "            num_sents = len(sentence_nodes_gnn)\n",
        "            if num_sents < self.max_sentences:\n",
        "                padding_gnn = torch.zeros(self.max_sentences - num_sents,\n",
        "                                           self.gnn_output_dim, device=sentence_nodes_gnn.device)\n",
        "                sentence_nodes_gnn = torch.cat([sentence_nodes_gnn, padding_gnn])\n",
        "                padding_bert = torch.zeros(self.max_sentences - num_sents,\n",
        "                                           self.hidden_size, device=sentence_nodes_bert.device)\n",
        "                sentence_nodes_bert = torch.cat([sentence_nodes_bert, padding_bert])\n",
        "            else:\n",
        "                sentence_nodes_gnn = sentence_nodes_gnn[:self.max_sentences]\n",
        "                sentence_nodes_bert = sentence_nodes_bert[:self.max_sentences]\n",
        "\n",
        "            # Hybrid\n",
        "            if self.use_hybrid:\n",
        "                claim_combined = torch.cat([claim_node_bert, claim_node_gnn], dim=-1)\n",
        "                sentence_combined = torch.cat([sentence_nodes_bert, sentence_nodes_gnn], dim=-1)\n",
        "            else:\n",
        "                claim_combined = claim_node_gnn\n",
        "                sentence_combined = sentence_nodes_gnn\n",
        "\n",
        "            label_logits_list.append(self.label_classifier(claim_combined))\n",
        "            evidence_logits_list.append(self.evidence_classifier(sentence_combined).squeeze(-1))\n",
        "\n",
        "        label_logits = torch.stack(label_logits_list)\n",
        "        evidence_logits = torch.stack(evidence_logits_list)\n",
        "\n",
        "        return label_logits, evidence_logits\n",
        "print(\"GNN model defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ytbj5BCCqn",
        "outputId": "ca67fd45-ed0a-4bcb-d8fd-4470cd5fbe10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN dataset class defined (FIXED: returns tokenized inputs, includes NEI)\n"
          ]
        }
      ],
      "source": [
        "# Dataset Class for GNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "import random\n",
        "\n",
        "class SciFactGNNDataset(Dataset):\n",
        "    \"\"\"Dataset that builds graphs for GNN training. Returns tokenized inputs for end-to-end training.\"\"\"\n",
        "\n",
        "    def __init__(self, claims, corpus, tokenizer, device,\n",
        "                 max_sentences=20, similarity_threshold=0.3, mode='train',\n",
        "                 negative_ratio=0.3, random_seed=42):\n",
        "        self.claims = claims\n",
        "        self.corpus = corpus\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.max_sentences = max_sentences\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.mode = mode\n",
        "        self.label_map = {'SUPPORT': 0, 'CONTRADICT': 1, 'NOT_ENOUGH_INFO': 2}\n",
        "        self.negative_ratio = negative_ratio\n",
        "\n",
        "        # Build examples list\n",
        "        self.examples = []\n",
        "\n",
        "        if mode == 'train':\n",
        "            # Positive examples: claims with evidence\n",
        "            claims_with_evidence = [c for c in claims if c.evidence and c.label]\n",
        "            for claim in claims_with_evidence:\n",
        "                if claim.cited_doc_ids:\n",
        "                    doc_id = int(claim.cited_doc_ids[0])\n",
        "                    if doc_id in corpus:\n",
        "                        self.examples.append({\n",
        "                            'claim': claim,\n",
        "                            'doc_id': doc_id,\n",
        "                            'is_negative': False\n",
        "                        })\n",
        "\n",
        "            # Negative examples: NEI claims with random documents\n",
        "            nei_claims = [c for c in claims if not c.evidence or c.label == 'NOT_ENOUGH_INFO']\n",
        "            num_negatives = int(len(claims_with_evidence) * negative_ratio)\n",
        "            available_doc_ids = list(corpus.keys())\n",
        "            random.seed(random_seed)\n",
        "\n",
        "            for i in range(num_negatives):\n",
        "                claim = random.choice(nei_claims)\n",
        "                cited_doc_ids = [int(d) for d in claim.cited_doc_ids] if claim.cited_doc_ids else []\n",
        "                candidate_docs = [d for d in available_doc_ids if d not in cited_doc_ids]\n",
        "                doc_id = random.choice(candidate_docs) if candidate_docs else random.choice(available_doc_ids)\n",
        "                self.examples.append({\n",
        "                    'claim': claim,\n",
        "                    'doc_id': doc_id,\n",
        "                    'is_negative': True\n",
        "                })\n",
        "\n",
        "            print(f\"Dataset: {len(self.examples)} examples ({len(claims_with_evidence)} positive, {num_negatives} negative)\")\n",
        "        else:\n",
        "            # Dev: all claims\n",
        "            for claim in claims:\n",
        "                if claim.cited_doc_ids:\n",
        "                    doc_id = int(claim.cited_doc_ids[0])\n",
        "                    if doc_id in corpus:\n",
        "                        self.examples.append({\n",
        "                            'claim': claim,\n",
        "                            'doc_id': doc_id,\n",
        "                            'is_negative': False\n",
        "                        })\n",
        "            print(f\"Dataset: {len(self.examples)} examples ({mode})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        claim = example['claim']\n",
        "        doc_id = example['doc_id']\n",
        "        is_negative = example['is_negative']\n",
        "\n",
        "        doc = self.corpus[doc_id]\n",
        "        sentences = doc.abstract[:self.max_sentences]\n",
        "\n",
        "        # Tokenize claim and sentences\n",
        "        claim_tokens = self.tokenizer(\n",
        "            claim.claim,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        sentence_tokens_list = []\n",
        "        for sent in sentences:\n",
        "            sent_tokens = self.tokenizer(\n",
        "                sent,\n",
        "                max_length=128,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            sentence_tokens_list.append(sent_tokens)\n",
        "\n",
        "        # Labels\n",
        "        if is_negative:\n",
        "            label = 2  # NOT_ENOUGH_INFO\n",
        "        else:\n",
        "            label = self.label_map.get(claim.label, 2)\n",
        "\n",
        "        # Evidence mask\n",
        "        evidence_mask = torch.zeros(self.max_sentences)\n",
        "        if not is_negative:\n",
        "            doc_id_str = str(doc.doc_id)\n",
        "            if doc_id_str in claim.evidence:\n",
        "                for ev_entry in claim.evidence[doc_id_str]:\n",
        "                    for sent_idx in ev_entry.get('sentences', []):\n",
        "                        if sent_idx < self.max_sentences:\n",
        "                            evidence_mask[sent_idx] = 1.0\n",
        "\n",
        "        return {\n",
        "            'claim_tokens': claim_tokens,\n",
        "            'sentence_tokens_list': sentence_tokens_list,\n",
        "            'num_sentences': len(sentences),\n",
        "            'similarity_threshold': self.similarity_threshold,\n",
        "            'claim_node_idx': 0,\n",
        "            'label': torch.tensor(label),\n",
        "            'evidence_mask': evidence_mask\n",
        "        }\n",
        "\n",
        "print(\"GNN dataset class defined (FIXED: returns tokenized inputs, includes NEI)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt9fUMR-CCqo",
        "outputId": "68097bff-58e6-4ceb-b44c-733d6f6fbd81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 114,976,260\n",
            "Trainable parameters: 114,976,260\n",
            "Dataset: 656 examples (505 positive, 151 negative)\n",
            "Dataset: 300 examples (dev)\n",
            "Training batches: 164\n",
            "Dev batches: 75\n",
            "Training setup complete\n"
          ]
        }
      ],
      "source": [
        "# Training Setup\n",
        "# Initialize model\n",
        "model = GNNClaimVerifier(\n",
        "    model_name=MODEL_NAME,\n",
        "    gnn_hidden_dim=GNN_HIDDEN_DIM,\n",
        "    gnn_num_layers=GNN_NUM_LAYERS,\n",
        "    gnn_num_heads=GNN_NUM_HEADS,\n",
        "    gnn_dropout=GNN_DROPOUT,\n",
        "    use_hybrid=USE_HYBRID\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SciFactGNNDataset(\n",
        "    train_claims, corpus, model.tokenizer, device,\n",
        "    max_sentences=MAX_SENTENCES,\n",
        "    similarity_threshold=SIMILARITY_THRESHOLD,\n",
        "    mode='train',\n",
        "    negative_ratio=NEGATIVE_RATIO\n",
        ")\n",
        "\n",
        "dev_dataset = SciFactGNNDataset(\n",
        "    dev_claims, corpus, model.tokenizer, device,\n",
        "    max_sentences=MAX_SENTENCES,\n",
        "    similarity_threshold=SIMILARITY_THRESHOLD,\n",
        "    mode='dev'\n",
        ")\n",
        "\n",
        "# Custom collate function\n",
        "def collate_graphs(batch):\n",
        "    claim_tokens_list = [item['claim_tokens'] for item in batch]\n",
        "    sentence_tokens_lists = [item['sentence_tokens_list'] for item in batch]\n",
        "    num_sentences_list = [item['num_sentences'] for item in batch]\n",
        "    similarity_thresholds = [item['similarity_threshold'] for item in batch]\n",
        "    labels = torch.stack([item['label'] for item in batch])\n",
        "    evidence_masks = torch.stack([item['evidence_mask'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'claim_tokens_list': claim_tokens_list,\n",
        "        'sentence_tokens_lists': sentence_tokens_lists,\n",
        "        'num_sentences_list': num_sentences_list,\n",
        "        'similarity_thresholds': similarity_thresholds,\n",
        "        'labels': labels,\n",
        "        'evidence_masks': evidence_masks\n",
        "    }\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_graphs)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_graphs)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Dev batches: {len(dev_loader)}\")\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "label_criterion = nn.CrossEntropyLoss()\n",
        "evidence_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(\"Training setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9bEZQOgCCqo",
        "outputId": "17a38aa3-27cf-4524-b6da-1a2d31d949be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/6: 100%|██████████| 164/164 [04:28<00:00,  1.64s/it, loss=1.8270, acc=48.5%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "  Loss: 2.0679\n",
            "  Label Accuracy: 48.48%\n",
            "  Saved: models/claim_verifier/gnn_epoch1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/6: 100%|██████████| 164/164 [04:12<00:00,  1.54s/it, loss=3.2523, acc=58.2%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "  Loss: 1.7405\n",
            "  Label Accuracy: 58.23%\n",
            "  Saved: models/claim_verifier/gnn_epoch2.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/6: 100%|██████████| 164/164 [04:16<00:00,  1.56s/it, loss=1.2993, acc=69.2%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Summary:\n",
            "  Loss: 1.3711\n",
            "  Label Accuracy: 69.21%\n",
            "  Saved: models/claim_verifier/gnn_epoch3.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/6: 100%|██████████| 164/164 [04:15<00:00,  1.56s/it, loss=0.6202, acc=78.2%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Summary:\n",
            "  Loss: 1.0205\n",
            "  Label Accuracy: 78.20%\n",
            "  Saved: models/claim_verifier/gnn_epoch4.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/6: 100%|██████████| 164/164 [04:15<00:00,  1.56s/it, loss=1.6540, acc=85.1%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Summary:\n",
            "  Loss: 0.7544\n",
            "  Label Accuracy: 85.06%\n",
            "  Saved: models/claim_verifier/gnn_epoch5.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/6: 100%|██████████| 164/164 [04:13<00:00,  1.55s/it, loss=0.1205, acc=88.3%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "  Loss: 0.5764\n",
            "  Label Accuracy: 88.26%\n",
            "  Saved: models/claim_verifier/gnn_epoch6.pt\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "model.train()\n",
        "os.makedirs('models/claim_verifier', exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        claim_tokens_list = batch['claim_tokens_list']\n",
        "        sentence_tokens_lists = batch['sentence_tokens_lists']\n",
        "        num_sentences_list = batch['num_sentences_list']\n",
        "        similarity_thresholds = batch['similarity_thresholds']\n",
        "        labels = batch['labels'].to(device)\n",
        "        evidence_masks = batch['evidence_masks'].to(device)\n",
        "\n",
        "        # Forward\n",
        "        label_logits, evidence_logits = model(\n",
        "            claim_tokens_list,\n",
        "            sentence_tokens_lists,\n",
        "            num_sentences_list,\n",
        "            similarity_thresholds\n",
        "        )\n",
        "\n",
        "        # Losses\n",
        "        label_loss = label_criterion(label_logits, labels)\n",
        "        evidence_loss = evidence_criterion(evidence_logits, evidence_masks)\n",
        "        loss = label_loss + EVIDENCE_LOSS_WEIGHT * evidence_loss\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stats\n",
        "        total_loss += loss.item()\n",
        "        pred = label_logits.argmax(dim=1)\n",
        "        correct += (pred == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  Label Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = f'models/claim_verifier/gnn_epoch{epoch+1}.pt'\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"  Saved: {checkpoint_path}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqQl1edyCCqo",
        "outputId": "4593a972-ebf8-464a-c6b2-9413b166e566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION ON DEV SET\n",
            "\n",
            "--- Testing threshold: 0.3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:39<00:00,  7.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.7994\n",
            "  F1:        0.8885\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1202\n",
            "  Recall:    0.2213\n",
            "  F1:        0.1558\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.35 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:35<00:00,  8.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.7906\n",
            "  F1:        0.8830\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1216\n",
            "  Recall:    0.2049\n",
            "  F1:        0.1526\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.4 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:36<00:00,  8.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.7699\n",
            "  F1:        0.8700\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1183\n",
            "  Recall:    0.1940\n",
            "  F1:        0.1470\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.45 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:35<00:00,  8.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.7552\n",
            "  F1:        0.8605\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1228\n",
            "  Recall:    0.1913\n",
            "  F1:        0.1496\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:36<00:00,  8.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.7345\n",
            "  F1:        0.8469\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1208\n",
            "  Recall:    0.1776\n",
            "  F1:        0.1438\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.55 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:36<00:00,  8.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.6932\n",
            "  F1:        0.8188\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1237\n",
            "  Recall:    0.1667\n",
            "  F1:        0.1420\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "--- Testing threshold: 0.6 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating predictions: 100%|██████████| 300/300 [00:36<00:00,  8.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.6667\n",
            "  F1:        0.8000\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1274\n",
            "  Recall:    0.1639\n",
            "  F1:        0.1434\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "THRESHOLD COMPARISON\n",
            "============================================================\n",
            "Threshold    Precision    Recall       F1          \n",
            "------------------------------------------------------------\n",
            "0.30         0.1202       0.2213       0.1558      \n",
            "0.35         0.1216       0.2049       0.1526      \n",
            "0.40         0.1183       0.1940       0.1470      \n",
            "0.45         0.1228       0.1913       0.1496      \n",
            "0.50         0.1208       0.1776       0.1438      \n",
            "0.55         0.1237       0.1667       0.1420      \n",
            "0.60         0.1274       0.1639       0.1434      \n",
            "\n",
            "============================================================\n",
            "BEST RESULTS\n",
            "============================================================\n",
            "Best Threshold: 0.3\n",
            "  Precision: 0.1202 (12.02%)\n",
            "  Recall:    0.2213 (22.13%)\n",
            "  F1:        0.1558 (15.58%)\n",
            "\n",
            "Baseline F1: 24.20%\n",
            "Improvement: -8.62%\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Evaluation: Generate Predictions\n",
        "import jsonlines\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch_geometric.data import Batch, Data\n",
        "\n",
        "def generate_predictions_gnn(claims, corpus, model, device, threshold=0.5):\n",
        "    \"\"\"Generate predictions using GNN model.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for claim in tqdm(claims, desc=\"Generating predictions\"):\n",
        "            if not hasattr(claim, 'cited_doc_ids') or not claim.cited_doc_ids:\n",
        "                predictions.append({'id': claim.id, 'label': 'NOT_ENOUGH_INFO', 'evidence': {}})\n",
        "                continue\n",
        "\n",
        "            doc_id = int(claim.cited_doc_ids[0])\n",
        "            if doc_id not in corpus:\n",
        "                predictions.append({'id': claim.id, 'label': 'NOT_ENOUGH_INFO', 'evidence': {}})\n",
        "                continue\n",
        "\n",
        "            doc = corpus[doc_id]\n",
        "            sentences = doc.abstract[:MAX_SENTENCES]\n",
        "\n",
        "            # Tokenize claim & sentences exactly like the training dataset does\n",
        "            claim_tokens = model.tokenizer(\n",
        "                claim.claim,\n",
        "                max_length=128,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            ).to(device)\n",
        "\n",
        "            sentence_tokens_list = []\n",
        "            for sent in sentences:\n",
        "                sent_tokens = model.tokenizer(\n",
        "                    sent,\n",
        "                    max_length=128,\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    return_tensors='pt'\n",
        "                ).to(device)\n",
        "                sentence_tokens_list.append(sent_tokens)\n",
        "\n",
        "            # Model expects lists of length batch_size (here = 1)\n",
        "            claim_tokens_list       = [claim_tokens]          # list with 1 element\n",
        "            sentence_tokens_lists   = [sentence_tokens_list]  # list of list\n",
        "\n",
        "            # Prepare additional args\n",
        "            num_sents = len(sentences)\n",
        "            num_sentences_list = [num_sents]\n",
        "            similarity_thresholds = torch.tensor([SIMILARITY_THRESHOLD],\n",
        "                                                 dtype=torch.float, device=device)\n",
        "\n",
        "            #  ACTUAL FORWARD CALL\n",
        "            label_logits, evidence_logits = model(\n",
        "                claim_tokens_list=claim_tokens_list,\n",
        "                sentence_tokens_lists=sentence_tokens_lists,\n",
        "                num_sentences_list=num_sentences_list,\n",
        "                similarity_thresholds=similarity_thresholds\n",
        "            )\n",
        "\n",
        "            # Get predictions\n",
        "            pred_label_idx = label_logits.argmax(dim=1).item()\n",
        "            label_map = {0: 'SUPPORT', 1: 'CONTRADICT', 2: 'NOT_ENOUGH_INFO'}\n",
        "            pred_label = label_map[pred_label_idx]\n",
        "\n",
        "            evidence_probs = torch.sigmoid(evidence_logits[0])\n",
        "            num_sents = len(sentences)\n",
        "            pred_evidence_sents = [i for i, prob in enumerate(evidence_probs[:num_sents])\n",
        "                                   if prob > threshold]\n",
        "\n",
        "            prediction = {'id': claim.id, 'label': pred_label, 'evidence': {}}\n",
        "            if pred_evidence_sents:\n",
        "                prediction['evidence'][str(doc_id)] = [{\n",
        "                    'sentences': pred_evidence_sents,\n",
        "                    'label': pred_label\n",
        "                }]\n",
        "            predictions.append(prediction)  # Always append here\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Test different thresholds\n",
        "print(\"EVALUATION ON DEV SET\")\n",
        "best_f1 = 0.0\n",
        "best_threshold = 0.5\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "threshold_results = []\n",
        "\n",
        "for threshold in [0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60]:\n",
        "    print(f\"\\n--- Testing threshold: {threshold} ---\")\n",
        "\n",
        "    predictions = generate_predictions_gnn(dev_claims, corpus, model, device, threshold)\n",
        "\n",
        "    # Save predictions\n",
        "    output_path = f'output/dev/gnn_thresh{int(threshold*100)}.jsonl'\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with jsonlines.open(output_path, 'w') as writer:\n",
        "        writer.write_all(predictions)\n",
        "\n",
        "    # Evaluate\n",
        "    result = subprocess.run(\n",
        "        ['python', 'src/evaluation/score_claims.py',\n",
        "         '--gold', 'data/scifact/data/claims_dev.jsonl',\n",
        "         '--predictions', output_path],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    print(result.stdout)\n",
        "\n",
        "    # Extract metrics\n",
        "    try:\n",
        "        lines = result.stdout.split('\\n')\n",
        "        precision = None\n",
        "        recall = None\n",
        "        f1 = None\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'Sentence-level' in line:\n",
        "                for j in range(i, min(i+10, len(lines))):\n",
        "                    if 'Precision:' in lines[j]:\n",
        "                        precision = float(lines[j].split('Precision:')[1].strip())\n",
        "                    elif 'Recall:' in lines[j]:\n",
        "                        recall = float(lines[j].split('Recall:')[1].strip())\n",
        "                    elif 'F1:' in lines[j]:\n",
        "                        f1_str = lines[j].split('F1:')[1].strip()\n",
        "                        f1 = float(f1_str)\n",
        "                        break\n",
        "                break\n",
        "\n",
        "        if f1 is not None:\n",
        "            threshold_results.append({\n",
        "                'threshold': threshold,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            })\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "                best_precision = precision if precision else 0.0\n",
        "                best_recall = recall if recall else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not parse metrics: {e}\")\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"THRESHOLD COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for result in threshold_results:\n",
        "    print(f\"{result['threshold']:<12.2f} {result['precision']:<12.4f} {result['recall']:<12.4f} {result['f1']:<12.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"BEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Threshold: {best_threshold}\")\n",
        "print(f\"  Precision: {best_precision:.4f} ({best_precision*100:.2f}%)\")\n",
        "print(f\"  Recall:    {best_recall:.4f} ({best_recall*100:.2f}%)\")\n",
        "print(f\"  F1:        {best_f1:.4f} ({best_f1*100:.2f}%)\")\n",
        "print(f\"\\nBaseline F1: 24.20%\")\n",
        "print(f\"Improvement: {best_f1*100 - 24.20:.2f}%\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go7L-Ag4CCqo"
      },
      "source": [
        "## Results Summary\n",
        "\n",
        "### Comparison with Baseline\n",
        "\n",
        "| Model | F1 | Precision | Recall | Notes |\n",
        "|-------|-----|-----------|--------|-------|\n",
        "| SciBERT Baseline | 24.20% | 19.09% | 33.06% | Milestone 2 |\n",
        "| **SciBERT + GNN** | **TBD** | **TBD** | **TBD** | This extension |\n",
        "\n",
        "### Analysis\n",
        "\n",
        "Results will be populated after training and evaluation.\n",
        "\n",
        "### Key Hyperparameters Used\n",
        "\n",
        "- GNN Hidden Dim: 256\n",
        "- GNN Layers: 2\n",
        "- GNN Heads: 4\n",
        "- Similarity Threshold: 0.3\n",
        "- Hybrid Mode: True\n",
        "- Learning Rate: 1e-4\n",
        "- Batch Size: 4\n",
        "\n",
        "### Expected vs Actual\n",
        "\n",
        "- **Expected**: +2-4% F1 improvement\n",
        "- **Actual**: TBD (will be updated after evaluation)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
