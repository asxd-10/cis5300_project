{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cklarO0STzQ3",
        "outputId": "194a6151-2be6-4553-ec7d-71fabbf6cfcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx-BhvPqUmqN",
        "outputId": "4e95e783-c37b-4bfb-9e6c-5bad01b6d446"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets jsonlines scikit-learn"
      ],
      "metadata": {
        "id": "vS3Y1iv3U5lr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf cis5300_project"
      ],
      "metadata": {
        "id": "n7x3Mde5gzVC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/asxd-10/cis5300_project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6InF-SbVHnE",
        "outputId": "53432fde-96d7-494a-8243-a242c3ec69ed"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cis5300_project'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 199 (delta 100), reused 81 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (199/199), 14.18 MiB | 15.60 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "try:\n",
        "    os.chdir('cis5300_project')\n",
        "    print(f\"Current Working Directory changed to: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The 'cis5300_project' directory was not found in the current location.\")\n",
        "sys.path.append('cis5300_project')\n",
        "from src.common.data_utils import load_claims, load_corpus\n",
        "\n",
        "print(\"Loading data\")\n",
        "train_claims = load_claims('data/scifact/data/claims_train.jsonl')\n",
        "dev_claims = load_claims('data/scifact/data/claims_dev.jsonl')\n",
        "corpus = load_corpus('data/scifact/data/corpus.jsonl')\n",
        "\n",
        "print(f\"{len(train_claims)} training claims\")\n",
        "print(f\"{len(dev_claims)} dev claims\")\n",
        "print(f\"{len(corpus)} documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USafeaiZW9YZ",
        "outputId": "ec0dd289-cc14-42eb-b337-74482d7a9098"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory changed to: /content/cis5300_project/cis5300_project/cis5300_project/cis5300_project/cis5300_project\n",
            "Loading data\n",
            "809 training claims\n",
            "300 dev claims\n",
            "5183 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.claim_verification.model import ClaimVerifier\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = ClaimVerifier()\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model loaded to GPU\")\n"
      ],
      "metadata": {
        "id": "Wmf-IP5jXAWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c8f4bb-2f8c-42c6-b35d-b0e06cfb3ff6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model loaded to GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "class SciFact_Dataset(Dataset):\n",
        "    def __init__(self, claims, corpus, tokenizer):\n",
        "        self.claims = claims\n",
        "        self.corpus = corpus\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Filter claims with evidence\n",
        "        self.valid_claims = [c for c in claims if c.evidence and c.label]\n",
        "        print(f\"  Using {len(self.valid_claims)}/{len(claims)} claims with evidence\")\n",
        "\n",
        "        self.label_map = {'SUPPORT': 0, 'CONTRADICT': 1, 'NOT_ENOUGH_INFO': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_claims)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim = self.valid_claims[idx]\n",
        "\n",
        "        # Get first evidence doc (keep as string for evidence lookup!)\n",
        "        doc_id_str = list(claim.evidence.keys())[0]  # Keep as STRING\n",
        "        doc_id_int = int(doc_id_str)  # Convert to INT for corpus lookup\n",
        "        doc = self.corpus[doc_id_int]\n",
        "\n",
        "        # Create input\n",
        "        text = claim.claim\n",
        "        num_sents = min(len(doc.abstract), 10)\n",
        "        for sent in doc.abstract[:num_sents]:\n",
        "            text += \" [SEP] \" + sent\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Label\n",
        "        label = self.label_map.get(claim.label, 2)\n",
        "\n",
        "        # FIXED: Create evidence mask using STRING key\n",
        "        evidence_mask = torch.zeros(20)\n",
        "\n",
        "        # Use STRING key to access evidence (CRITICAL!)\n",
        "        if doc_id_str in claim.evidence:\n",
        "            for ev_entry in claim.evidence[doc_id_str]:\n",
        "                for sent_idx in ev_entry['sentences']:\n",
        "                    if sent_idx < num_sents:\n",
        "                        evidence_mask[sent_idx] = 1.0\n",
        "\n",
        "        # Create sentence positions\n",
        "        sentence_positions = torch.zeros(20, dtype=torch.long)\n",
        "        claim_tokens = self.tokenizer.encode(claim.claim, add_special_tokens=True)\n",
        "        current_pos = len(claim_tokens)\n",
        "\n",
        "        for i in range(num_sents):\n",
        "            sent_tokens = self.tokenizer.encode(doc.abstract[i], add_special_tokens=False)\n",
        "            sentence_positions[i] = current_pos + len(sent_tokens)\n",
        "            current_pos += len(sent_tokens) + 1\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'sentence_positions': sentence_positions,\n",
        "            'label': torch.tensor(label),\n",
        "            'evidence_mask': evidence_mask\n",
        "        }"
      ],
      "metadata": {
        "id": "M2Q8fI39XMEN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing fixed dataset...\")\n",
        "train_dataset = SciFact_Dataset(train_claims, corpus, model.tokenizer)\n",
        "\n",
        "# Check first example\n",
        "test_sample = train_dataset[0]\n",
        "print(f\"\\nEvidence mask: {test_sample['evidence_mask'][:10]}\")\n",
        "print(f\"Has evidence: {test_sample['evidence_mask'].sum().item() > 0}\")\n",
        "print(f\"Num evidence sents: {int(test_sample['evidence_mask'].sum().item())}\")\n",
        "\n",
        "# Check batch\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "batch = next(iter(train_loader))\n",
        "num_with_ev = (batch['evidence_mask'].sum(dim=1) > 0).sum().item()\n",
        "print(f\"\\nBatch: {num_with_ev}/8 examples have evidence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxGl47WI9MnI",
        "outputId": "0ce8736c-5b1b-4798-d618-58e3781fee93"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing fixed dataset...\n",
            "  Using 505/809 claims with evidence\n",
            "\n",
            "Evidence mask: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
            "Has evidence: True\n",
            "Num evidence sents: 1\n",
            "\n",
            "Batch: 7/8 examples have evidence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "print(\"Creating dataset\")\n",
        "train_dataset = SciFact_Dataset(train_claims, corpus, model.tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "print(f\"Dataset ready: {len(train_dataset)} examples, {len(train_loader)} batches\")"
      ],
      "metadata": {
        "id": "IGCdZjQFXYrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299ac3f1-157a-428b-b0d8-d762aef5f569"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset\n",
            "  Using 505/809 claims with evidence\n",
            "Dataset ready: 505 examples, 64 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "yq_6fgUOYc33"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking evidence masks...\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nBatch shapes:\")\n",
        "print(f\"  evidence_mask: {sample_batch['evidence_mask'].shape}\")\n",
        "print(f\"  sentence_positions: {sample_batch['sentence_positions'].shape}\")\n",
        "\n",
        "print(f\"\\nFirst example:\")\n",
        "print(f\"  Evidence mask: {sample_batch['evidence_mask'][0]}\")\n",
        "print(f\"  Sentence positions: {sample_batch['sentence_positions'][0]}\")\n",
        "\n",
        "# Count how many have evidence\n",
        "num_with_evidence = (sample_batch['evidence_mask'].sum(dim=1) > 0).sum().item()\n",
        "print(f\"\\nExamples with evidence: {num_with_evidence}/{sample_batch['evidence_mask'].size(0)}\")\n",
        "\n",
        "# Check if sentence positions are reasonable\n",
        "print(f\"Sentence position range: {sample_batch['sentence_positions'].min()}-{sample_batch['sentence_positions'].max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229dBI9y66-U",
        "outputId": "67263c70-37a4-41d3-de78-c43480cf7d9d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking evidence masks...\n",
            "\n",
            "Batch shapes:\n",
            "  evidence_mask: torch.Size([8, 20])\n",
            "  sentence_positions: torch.Size([8, 20])\n",
            "\n",
            "First example:\n",
            "  Evidence mask: tensor([1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "  Sentence positions: tensor([ 46, 103, 178, 227, 275, 335, 385,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0])\n",
            "\n",
            "Examples with evidence: 5/8\n",
            "Sentence position range: 0-385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING\")\n",
        "\n",
        "model.train()\n",
        "num_epochs = 8\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        # Move to GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        sentence_positions = batch['sentence_positions'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        evidence_mask = batch['evidence_mask'].to(device)\n",
        "\n",
        "        # Forward with evidence prediction\n",
        "        label_logits, evidence_logits = model(input_ids, attention_mask, sentence_positions)\n",
        "\n",
        "        # Multi-task loss\n",
        "        label_loss = criterion(label_logits, labels)\n",
        "        evidence_loss = nn.BCEWithLogitsLoss()(evidence_logits, evidence_mask)\n",
        "        loss = label_loss + 2.0 * evidence_loss  # 0.5 or 1.0 weight on evidence\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stats\n",
        "        total_loss += loss.item()\n",
        "        pred = label_logits.argmax(dim=1)\n",
        "        correct += (pred == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        evidence_pred = (torch.sigmoid(evidence_logits) > 0.5).float()\n",
        "        # Only count non-padding positions\n",
        "        valid_positions = (sentence_positions > 0).float() # All positions\n",
        "        correct_per_example = ((evidence_pred == evidence_mask) * valid_positions).sum(dim=1)\n",
        "        total_per_example = valid_positions.sum(dim=1)\n",
        "        evidence_acc = (correct_per_example / total_per_example).mean().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "          'loss': f'{loss.item():.4f}',\n",
        "          'acc': f'{100*correct/total:.1f}%',\n",
        "          'ev_acc': f'{100*evidence_acc:.1f}%'\n",
        "      })\n",
        "\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Save checkpoint to Google Drive\n",
        "    checkpoint_path = f'models/claim_verifier/model_epoch{epoch+1}.pt'\n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"  Saved: {checkpoint_path}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ],
      "metadata": {
        "id": "ARacI9MdYfjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cdcf7cb-22c7-4a1d-e27e-5fadb3e287a3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|██████████| 64/64 [00:53<00:00,  1.20it/s, loss=1.5078, acc=63.8%, ev_acc=87.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "  Loss: 1.8233\n",
            "  Accuracy: 63.76%\n",
            "  Saved: models/claim_verifier/model_epoch1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|██████████| 64/64 [00:52<00:00,  1.21it/s, loss=1.6988, acc=65.7%, ev_acc=62.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "  Loss: 1.7307\n",
            "  Accuracy: 65.74%\n",
            "  Saved: models/claim_verifier/model_epoch2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|██████████| 64/64 [00:52<00:00,  1.21it/s, loss=2.2979, acc=67.5%, ev_acc=90.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 Summary:\n",
            "  Loss: 1.6591\n",
            "  Accuracy: 67.52%\n",
            "  Saved: models/claim_verifier/model_epoch3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|██████████| 64/64 [00:52<00:00,  1.22it/s, loss=1.3389, acc=73.5%, ev_acc=50.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 Summary:\n",
            "  Loss: 1.5087\n",
            "  Accuracy: 73.47%\n",
            "  Saved: models/claim_verifier/model_epoch4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|██████████| 64/64 [00:52<00:00,  1.21it/s, loss=1.0781, acc=81.6%, ev_acc=90.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 Summary:\n",
            "  Loss: 1.3201\n",
            "  Accuracy: 81.58%\n",
            "  Saved: models/claim_verifier/model_epoch5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|██████████| 64/64 [00:53<00:00,  1.20it/s, loss=4.5197, acc=86.7%, ev_acc=87.5%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 Summary:\n",
            "  Loss: 1.2565\n",
            "  Accuracy: 86.73%\n",
            "  Saved: models/claim_verifier/model_epoch6.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|██████████| 64/64 [00:52<00:00,  1.22it/s, loss=1.0620, acc=91.3%, ev_acc=85.7%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 Summary:\n",
            "  Loss: 1.0708\n",
            "  Accuracy: 91.29%\n",
            "  Saved: models/claim_verifier/model_epoch7.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|██████████| 64/64 [00:53<00:00,  1.21it/s, loss=1.0213, acc=93.9%, ev_acc=100.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 Summary:\n",
            "  Loss: 0.9679\n",
            "  Accuracy: 93.86%\n",
            "  Saved: models/claim_verifier/model_epoch8.pt\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION ON DEV SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.eval()\n",
        "dev_dataset = SciFact_Dataset(dev_claims, corpus, model.tokenizer)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        label_logits, _ = model(input_ids, attention_mask, sentence_positions=None)\n",
        "        pred = label_logits.argmax(dim=1)\n",
        "\n",
        "        correct += (pred == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nDev Set Label Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"(Simple baseline was close to 0%, so anything is better!)\")"
      ],
      "metadata": {
        "id": "QHrL7lSxY3ne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2f4475-5ef3-4e2c-cd58-487b290786b3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION ON DEV SET\n",
            "============================================================\n",
            "  Using 188/300 claims with evidence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 12/12 [00:06<00:00,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dev Set Label Accuracy: 64.89%\n",
            "(Simple baseline was close to 0%, so anything is better!)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THRESHOLD EXPERIMENT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING DIFFERENT THRESHOLDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for threshold in [0.30, 0.35, 0.40, 0.50, 0.55]:\n",
        "    print(f\"\\n--- Threshold: {threshold} ---\")\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for claim in tqdm(dev_claims, desc=f\"Threshold {threshold}\", leave=False):\n",
        "            if not hasattr(claim, 'cited_doc_ids') or not claim.cited_doc_ids:\n",
        "                predictions.append({'id': claim.id, 'label': 'NOT_ENOUGH_INFO', 'evidence': {}})\n",
        "                continue\n",
        "\n",
        "            doc_id = int(claim.cited_doc_ids[0])\n",
        "            if doc_id not in corpus:\n",
        "                predictions.append({'id': claim.id, 'label': 'NOT_ENOUGH_INFO', 'evidence': {}})\n",
        "                continue\n",
        "\n",
        "            doc = corpus[doc_id]\n",
        "            text = claim.claim\n",
        "            num_sents = min(len(doc.abstract), 10)\n",
        "            for sent in doc.abstract[:num_sents]:\n",
        "                text += \" [SEP] \" + sent\n",
        "\n",
        "            encoding = model.tokenizer(text, max_length=512, padding='max_length',\n",
        "                                      truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "            sentence_positions = torch.zeros(1, 20, dtype=torch.long).to(device)\n",
        "            claim_tokens = model.tokenizer.encode(claim.claim, add_special_tokens=True)\n",
        "            current_pos = len(claim_tokens)\n",
        "\n",
        "            for i in range(num_sents):\n",
        "                sent_tokens = model.tokenizer.encode(doc.abstract[i], add_special_tokens=False)\n",
        "                sentence_positions[0, i] = current_pos + len(sent_tokens)\n",
        "                current_pos += len(sent_tokens) + 1\n",
        "\n",
        "            label_logits, evidence_logits = model(encoding['input_ids'],\n",
        "                                                   encoding['attention_mask'],\n",
        "                                                   sentence_positions)\n",
        "\n",
        "            pred_label_idx = label_logits.argmax(dim=1).item()\n",
        "            label_map = {0: 'SUPPORT', 1: 'CONTRADICT', 2: 'NOT_ENOUGH_INFO'}\n",
        "            pred_label = label_map[pred_label_idx]\n",
        "\n",
        "            evidence_probs = torch.sigmoid(evidence_logits[0])\n",
        "            pred_evidence_sents = [i for i, prob in enumerate(evidence_probs[:num_sents])\n",
        "                                   if prob > threshold]\n",
        "\n",
        "            prediction = {'id': claim.id, 'label': pred_label, 'evidence': {}}\n",
        "            if pred_evidence_sents:\n",
        "                prediction['evidence'][str(doc_id)] = [{\n",
        "                    'sentences': pred_evidence_sents,\n",
        "                    'label': pred_label\n",
        "                }]\n",
        "            else:\n",
        "                prediction['label'] = 'NOT_ENOUGH_INFO'\n",
        "\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    # Save and evaluate\n",
        "    output_path = f'output/dev/scibert_thresh{int(threshold*100)}.jsonl'\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    with jsonlines.open(output_path, 'w') as writer:\n",
        "        writer.write_all(predictions)\n",
        "\n",
        "    # Quick eval\n",
        "    !python src/evaluation/score_claims.py \\\n",
        "      --gold data/scifact/data/claims_dev.jsonl \\\n",
        "      --predictions {output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H-opkcdnXEo",
        "outputId": "7846870a-102e-4443-8612-8675370dfb65"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING DIFFERENT THRESHOLDS\n",
            "============================================================\n",
            "\n",
            "--- Threshold: 0.3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.8230\n",
            "  F1:        0.9029\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1779\n",
            "  Recall:    0.3607\n",
            "  F1:        0.2383\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            " Retrieval is excellent (oracle or near-oracle)\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "--- Threshold: 0.35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.8142\n",
            "  F1:        0.8976\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1822\n",
            "  Recall:    0.3579\n",
            "  F1:        0.2415\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "--- Threshold: 0.4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.8083\n",
            "  F1:        0.8940\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1838\n",
            "  Recall:    0.3525\n",
            "  F1:        0.2416\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "--- Threshold: 0.5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.8024\n",
            "  F1:        0.8903\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1863\n",
            "  Recall:    0.3333\n",
            "  F1:        0.2390\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n",
            "\n",
            "--- Threshold: 0.55 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLAIM VERIFICATION EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "  Gold claims: 300\n",
            "  Predictions: 300\n",
            "\n",
            "Computing metrics...\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "\n",
            "Abstract-level (Retrieval):\n",
            "  Precision: 1.0000\n",
            "  Recall:    0.8024\n",
            "  F1:        0.8903\n",
            "\n",
            "Sentence-level (Evidence + Label): PRIMARY METRIC\n",
            "  Precision: 0.1909\n",
            "  Recall:    0.3306\n",
            "  F1:        0.2420\n",
            "\n",
            "Label-only:\n",
            "  Accuracy:  0.0000\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  Retrieval is working reasonably\n",
            "  Evidence extraction is improving but below target\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "# Check gold format\n",
        "with jsonlines.open('data/scifact/data/claims_dev.jsonl') as f:\n",
        "    for claim in f:\n",
        "        if 'evidence' in claim and claim['evidence']:\n",
        "            print(\"Sample gold evidence:\")\n",
        "            print(f\"  Keys: {list(claim['evidence'].keys())}\")\n",
        "            print(f\"  Key type: {type(list(claim['evidence'].keys())[0])}\")\n",
        "            print(f\"  Full evidence: {claim['evidence']}\")\n",
        "            break\n",
        "\n",
        "# # Check your prediction format\n",
        "# with jsonlines.open('output/dev/scibert_predictions.jsonl') as f:\n",
        "#     for pred in f:\n",
        "#         if pred['evidence']:\n",
        "#             print(\"\\nSample prediction evidence:\")\n",
        "#             print(f\"  Keys: {list(pred['evidence'].keys())}\")\n",
        "#             print(f\"  Key type: {type(list(pred['evidence'].keys())[0])}\")\n",
        "#             print(f\"  Full evidence: {pred['evidence']}\")\n",
        "#             break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OisI5Jh-Re_b",
        "outputId": "03c5b81e-d77c-4520-c309-82d4448b91e1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample gold evidence:\n",
            "  Keys: ['14717500']\n",
            "  Key type: <class 'str'>\n",
            "  Full evidence: {'14717500': [{'sentences': [2, 5], 'label': 'SUPPORT'}, {'sentences': [7], 'label': 'SUPPORT'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug: Check what labels you're actually predicting\n",
        "# with jsonlines.open('output/dev/scibert_predictions.jsonl') as f:\n",
        "#     sample_preds = [p for p in f][:5]\n",
        "#     print(\"\\nSample predictions:\")\n",
        "#     for p in sample_preds:\n",
        "#         print(f\"  ID {p['id']}: label='{p['label']}', evidence={bool(p['evidence'])}\")\n",
        "\n",
        "# Check gold format\n",
        "with jsonlines.open('data/scifact/data/claims_dev.jsonl') as f:\n",
        "    sample_gold = [c for c in f][:5]\n",
        "    print(\"\\nSample gold:\")\n",
        "    for g in sample_gold:\n",
        "        print(f\"  ID {g['id']}: label='{g.get('label', 'MISSING')}'\")"
      ],
      "metadata": {
        "id": "t1QgHHghlbH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b21cc7-bbad-41f2-ae88-c6ac34111ec2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample gold:\n",
            "  ID 1: label='MISSING'\n",
            "  ID 3: label='MISSING'\n",
            "  ID 5: label='MISSING'\n",
            "  ID 13: label='MISSING'\n",
            "  ID 36: label='MISSING'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_aQ6Jw36nw5I"
      },
      "execution_count": 83,
      "outputs": []
    }
  ]
}