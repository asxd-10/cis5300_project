{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7b77f2-b7e2-4257-8c4f-832502cc3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashay\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.36.0 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e36d759-3339-44e5-9c94-67966f1ed729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ff5baac-b50a-4816-a0e1-06b2bfd9f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch version is < 2.6\n",
      "   Loading models with safetensors instead...\n"
     ]
    }
   ],
   "source": [
    "torch_version = tuple(int(x) for x in torch.__version__.split('.')[:2])\n",
    "if torch_version < (2, 6):\n",
    "    print(\"\\nPyTorch version is < 2.6\")\n",
    "    print(\"   Loading models with safetensors instead...\")\n",
    "    USE_SAFETENSORS = True\n",
    "else:\n",
    "    print(\"PyTorch version is compatible\")\n",
    "    USE_SAFETENSORS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7966fb1e-e4e9-46a3-895c-5a801c19aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from src.common.data_utils import load_claims, load_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fed9d8-da3e-4513-afce-08d87bc5732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 809 claims, 5183 docs\n"
     ]
    }
   ],
   "source": [
    "train_claims = load_claims('../data/scifact/data/claims_train.jsonl')\n",
    "corpus = load_corpus('../data/scifact/data/corpus.jsonl')\n",
    "print(f\"Loaded {len(train_claims)} claims, {len(corpus)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ace172b-4b86-410e-8846-ac014c61f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instance(claim, corpus):\n",
    "    \"\"\"Convert claim to model input.\"\"\"\n",
    "    # Use oracle retrieval (gold evidence docs)\n",
    "    if claim.evidence:\n",
    "        doc_id = list(claim.evidence.keys())[0]\n",
    "        doc = corpus[doc_id]\n",
    "        \n",
    "        # Build input text\n",
    "        text = claim.claim\n",
    "        for sent in doc.abstract:\n",
    "            text += \" [SEP] \" + sent\n",
    "        \n",
    "        # Get gold evidence mask\n",
    "        evidence_mask = torch.zeros(len(doc.abstract))\n",
    "        for ev_list in claim.evidence.values():\n",
    "            for ev in ev_list:\n",
    "                for sent_idx in ev['sentences']:\n",
    "                    if sent_idx < len(doc.abstract):\n",
    "                        evidence_mask[sent_idx] = 1\n",
    "        \n",
    "        # Get label\n",
    "        label_map = {'SUPPORT': 0, 'CONTRADICT': 1, 'NOT_ENOUGH_INFO': 2}\n",
    "        label = label_map[claim.label]\n",
    "        \n",
    "        return text, label, evidence_mask\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85da857-7dba-4f52-92d1-a0ea6e0a4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = create_training_instance(train_claims[0], corpus)\n",
    "if example:\n",
    "    text, label, ev_mask = example\n",
    "    print(f\"Input text (first 200 chars): {text[:200]}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Evidence mask: {ev_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a10c8b61-7446-449c-9c75-90f9ebf2f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a44687c842448b8396764d6f1953cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4710745e5b104d1494e8ba0fe5ac6f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a119757edc33494aa7f2a7c8537af37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3180f83e5817457fb309a86f28eec2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flax_model.msgpack:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m snapshot_download(\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/scibert_scivocab_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m         ignore_patterns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Ignore .bin files, use .safetensors\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(cache_dir)\n\u001b[1;32m---> 19\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(cache_dir)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallenai/scibert_scivocab_uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   5041\u001b[0m     (\n\u001b[0;32m   5042\u001b[0m         model,\n\u001b[0;32m   5043\u001b[0m         missing_keys,\n\u001b[0;32m   5044\u001b[0m         unexpected_keys,\n\u001b[0;32m   5045\u001b[0m         mismatched_keys,\n\u001b[0;32m   5046\u001b[0m         offload_index,\n\u001b[0;32m   5047\u001b[0m         error_msgs,\n\u001b[1;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   5049\u001b[0m         model,\n\u001b[0;32m   5050\u001b[0m         state_dict,\n\u001b[0;32m   5051\u001b[0m         checkpoint_files,\n\u001b[0;32m   5052\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   5053\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   5054\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   5055\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   5056\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   5057\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   5058\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   5059\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[0;32m   5060\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[0;32m   5061\u001b[0m         key_mapping\u001b[38;5;241m=\u001b[39mkey_mapping,\n\u001b[0;32m   5062\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   5063\u001b[0m     )\n\u001b[0;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:5316\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   5313\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   5314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5315\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 5316\u001b[0m         load_state_dict(checkpoint_files[\u001b[38;5;241m0\u001b[39m], map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39mweights_only)\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   5317\u001b[0m     )\n\u001b[0;32m   5319\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[0;32m   5320\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:508\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m--> 508\u001b[0m     check_torch_load_is_safe()\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1647\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1652\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "#Load SciBERT \n",
    "sys.path.append('..')\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Loading SciBERT...\")\n",
    "\n",
    "# Use safetensors if PyTorch version < 2.6\n",
    "if USE_SAFETENSORS:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    import os\n",
    "    \n",
    "    # Download model with safetensors\n",
    "    cache_dir = snapshot_download(\n",
    "        \"allenai/scibert_scivocab_uncased\",\n",
    "        ignore_patterns=[\"*.bin\"]  # Ignore .bin files, use .safetensors\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
    "    model = AutoModel.from_pretrained(cache_dir)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "print(f\"âœ“ Loaded SciBERT\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Num layers: {model.config.num_hidden_layers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
